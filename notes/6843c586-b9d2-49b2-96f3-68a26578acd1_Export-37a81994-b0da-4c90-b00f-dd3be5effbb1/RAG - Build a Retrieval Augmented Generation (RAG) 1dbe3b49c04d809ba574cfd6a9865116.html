<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>RAG - Build a Retrieval Augmented Generation (RAG) App: Part 1 | �…</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1dbe3b49-c04d-809b-a574-cfd6a9865116" class="page sans"><header><h1 class="page-title">RAG - <a href="https://python.langchain.com/docs/tutorials/rag/">https://python.langchain.com/docs/tutorials/rag/</a></h1><p class="page-description"></p></header><div class="page-body"><p id="1dbe3b49-c04d-8087-b664-ebfcfbb4b01f" class="">
</p><ul id="1dbe3b49-c04d-8095-afeb-ec9bba527fc1" class="bulleted-list"><li style="list-style-type:disc">Implement and explore the concepts that either new or for revision purpose</li></ul><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Part 1</summary><div class="indented"><ul id="1dbe3b49-c04d-806e-ab38-d6b23e9615c5" class="bulleted-list"><li style="list-style-type:disc">rag implementation for q and a over text source</li></ul><ul id="1dbe3b49-c04d-8061-b3d9-fd90de59b452" class="bulleted-list"><li style="list-style-type:disc">using langsmith for tracing</li></ul><p id="1dbe3b49-c04d-800b-832e-f93330e86389" class="">
</p><h3 id="1dbe3b49-c04d-80fb-92be-f6c32bff42fb" class="">Retrieval Techniques</h3><ul id="1dbe3b49-c04d-8010-9159-d571d7c16606" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/retrieval/">https://python.langchain.com/docs/concepts/retrieval/</a></li></ul><ul id="1dbe3b49-c04d-80bf-a50c-cee88227b5cb" class="bulleted-list"><li style="list-style-type:disc">Prerequisites<details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/embedding_models/">https://python.langchain.com/docs/concepts/embedding_models/</a></summary><div class="indented"><ul id="1dbe3b49-c04d-80c5-a68f-d64c596c42d7" class="bulleted-list"><li style="list-style-type:disc">Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text&#x27;s semantic meaning. </li></ul><ul id="1dbe3b49-c04d-8010-b53e-ead4066ee36c" class="bulleted-list"><li style="list-style-type:disc">Leaderboard for a standard benchmark - Massive Text Embedding Benchmark (MTEB) <a href="https://huggingface.co/blog/mteb">here</a> for objective comparisons.</li></ul><ul id="1dbe3b49-c04d-80e3-9895-dcd3dba86265" class="bulleted-list"><li style="list-style-type:disc">BERT used transformer architecture to turn text inputs into vector embedding which provided gains in many of the NLP tasks<ul id="1dbe3b49-c04d-8066-9a22-e2f64f39eaa2" class="bulleted-list"><li style="list-style-type:circle">It was not optimized for sentecen generation tasks.</li></ul><ul id="1dbe3b49-c04d-8071-9916-d7c78398e642" class="bulleted-list"><li style="list-style-type:circle">S-bERT was created.</li></ul></li></ul><ul id="1dbe3b49-c04d-800e-82aa-e76f5e2c4df0" class="bulleted-list"><li style="list-style-type:disc">BERT - <a href="https://www.nvidia.com/en-us/glossary/bert/">https://www.nvidia.com/en-us/glossary/bert/</a><ul id="1dbe3b49-c04d-8075-9822-d8c77a041cb9" class="bulleted-list"><li style="list-style-type:circle">Bidirectional Encoder Representations from Transformers (BERT)</li></ul><ul id="1dbe3b49-c04d-8022-87ec-f3c73b89ef1e" class="bulleted-list"><li style="list-style-type:circle">About left to right, and right to left processing by elMO which processing the entire sentence at one in both directions but does not combine it. whereas BERT unifies it, which leads to better contextual representation.</li></ul><ul id="1dbe3b49-c04d-80fb-a83f-ca0ad3987e1f" class="bulleted-list"><li style="list-style-type:circle">One of the key things in BERT, is language masking where it deletes or masks some of the words.</li></ul><ul id="1dbe3b49-c04d-809b-8954-fc6adb0ab98e" class="bulleted-list"><li style="list-style-type:circle">BERT’s developers solved this problem by masking predicted words as well as other random words in the corpus. BERT also uses a simple training technique of trying to predict whether, given two sentences A and B, B is the antecedent of A or a random sentence.</li></ul><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Directional Processing by elMO and BERT dimensionality of embeddings</summary><div class="indented"><h3 id="1dbe3b49-c04d-803a-a125-d22ad36a3699" class="">1. <strong>Left-to-Right and Right-to-Left Contexts</strong></h3><p id="1dbe3b49-c04d-8015-98be-cdc05c2a716a" class="">In the context of language models like ELMo and BERT, the terms <strong>left-to-right</strong> and <strong>right-to-left</strong> refer to how the model processes the sequence of words in a sentence during training or inference.</p><ul id="1dbe3b49-c04d-80e2-bdf8-e87cf78a9fb6" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-Right</strong> means the model processes the sentence from the <strong>first word</strong> to the <strong>last word</strong>. It only considers words that come before a given word when generating its representation. For example, if the model is processing the word &quot;bat&quot; in the sentence &quot;I hit the ball with a bat,&quot; it would process the words &quot;I hit the ball with a&quot; first, before considering &quot;bat.&quot;</li></ul><ul id="1dbe3b49-c04d-80a8-ac10-e96529c34f38" class="bulleted-list"><li style="list-style-type:disc"><strong>Right-to-Left</strong> means the model processes the sentence from the <strong>last word</strong> to the <strong>first word</strong>. It only considers words that come after a given word when generating its representation. For example, in the same sentence, the model would first process the words &quot;a bat&quot; and then &quot;with the ball hit I&quot; when processing the word &quot;bat.&quot;</li></ul><p id="1dbe3b49-c04d-80a7-bcfb-f080fcb2a011" class="">Here’s a simple illustration:</p><ul id="1dbe3b49-c04d-8062-b27a-d9c074d17eb7" class="bulleted-list"><li style="list-style-type:disc"><strong>Sentence</strong>: &quot;The bat flew away.&quot;</li></ul><ul id="1dbe3b49-c04d-8038-9237-d9f62d2ba0d9" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-right processing</strong> (For &quot;bat&quot;): It would consider: &quot;The&quot; → &quot;bat&quot; → &quot;flew&quot; → &quot;away.&quot;</li></ul><ul id="1dbe3b49-c04d-803b-8d57-ee2c92437ef8" class="bulleted-list"><li style="list-style-type:disc"><strong>Right-to-left processing</strong> (For &quot;bat&quot;): It would consider: &quot;away&quot; → &quot;flew&quot; → &quot;bat&quot; → &quot;The.&quot;</li></ul><p id="1dbe3b49-c04d-808d-9454-cd3ea4695f5c" class=""><strong>ELMo</strong> used separate models for these two directions, while <strong>BERT</strong> uses <strong>bidirectional</strong> processing, meaning it considers both the left-to-right and right-to-left context <strong>at the same time</strong>. This gives a more holistic view of the word in the sentence.</p><h3 id="1dbe3b49-c04d-807e-a08e-d88ff6380bd6" class="">2. <strong>What Drives the Size of Vector Representations?</strong></h3><p id="1dbe3b49-c04d-801b-8500-d0c1d4ba9fb1" class="">The <strong>size</strong> of vector representations, also known as the <strong>dimensionality</strong> of the embedding, is determined by the model architecture and the <strong>number of parameters</strong> chosen for the model.</p><p id="1dbe3b49-c04d-80b3-9163-f172284cd0d4" class="">Several factors that influence the size of vector representations:</p><ul id="1dbe3b49-c04d-80fd-a738-eb1a71fefd03" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Architecture</strong>:<ul id="1dbe3b49-c04d-8018-b2f7-d4fc7b60ab40" class="bulleted-list"><li style="list-style-type:circle">In <strong>Word2Vec</strong> or <strong>GloVe</strong>, the size of the word vector is a hyperparameter you can set. Common choices are 50, 100, 200, or 300 dimensions.</li></ul><ul id="1dbe3b49-c04d-8096-90b4-ef25f41e3f82" class="bulleted-list"><li style="list-style-type:circle">In <strong>ELMo</strong> and <strong>BERT</strong>, the size is determined by the model architecture. For example, BERT-base uses vectors of size <strong>768</strong>, and BERT-large uses vectors of size <strong>1024</strong>.</li></ul></li></ul><ul id="1dbe3b49-c04d-80ff-a466-f673442882b2" class="bulleted-list"><li style="list-style-type:disc"><strong>Hidden Layers</strong>:<ul id="1dbe3b49-c04d-800b-b78c-d5bf798fda41" class="bulleted-list"><li style="list-style-type:circle"><strong>BERT</strong>, being a transformer model, has multiple <strong>layers</strong> (e.g., 12 layers for BERT-base). Each layer processes information differently, and the output at each layer can be represented as a vector.</li></ul><ul id="1dbe3b49-c04d-8003-a61a-f5c50125beb8" class="bulleted-list"><li style="list-style-type:circle">The size of these vectors typically corresponds to the <strong>number of hidden units</strong> in the transformer model. For example, BERT-base has <strong>768 hidden units</strong> per layer, and the final word embedding output (after processing) will also be a vector of size 768.</li></ul></li></ul><ul id="1dbe3b49-c04d-80d6-b393-cbe64016f278" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Capacity</strong>:<ul id="1dbe3b49-c04d-8084-b0cd-d2c9c11ed3c1" class="bulleted-list"><li style="list-style-type:circle">Larger models (like <strong>BERT-large</strong>) tend to have larger vector sizes (e.g., 1024) to capture more complex patterns, but they also require more computation and memory.</li></ul><ul id="1dbe3b49-c04d-8051-bff5-d7c8fc038e9c" class="bulleted-list"><li style="list-style-type:circle">Smaller models may use fewer dimensions (e.g., 256, 512) for efficiency but may not capture as much detail in the word representations.</li></ul></li></ul><h3 id="1dbe3b49-c04d-8062-b91d-e75b328cba7c" class="">In summary:</h3><ul id="1dbe3b49-c04d-8046-9a2d-fc16dd3dccb9" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-Right and Right-to-Left</strong> refer to how the model processes words sequentially from different directions. ELMo used these separately, while BERT combines both for a deeper understanding.</li></ul><ul id="1dbe3b49-c04d-8000-80c3-cc467efcb54d" class="bulleted-list"><li style="list-style-type:disc"><strong>Vector Size</strong>: The size of the vector representation (like 768 or 1024 in BERT) is determined by the architecture of the model (number of layers, hidden units, etc.). It impacts the model&#x27;s ability to capture complex patterns but also affects computational requirements.</li></ul></div></details><p id="1dbe3b49-c04d-80a6-a911-d4c9655ca3fe" class="">
</p><ul id="1dbe3b49-c04d-80eb-ae11-cb337d485602" class="bulleted-list"><li style="list-style-type:circle">more detail - <a href="https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/">https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/</a></li></ul><ul id="1dbe3b49-c04d-8055-ab42-f956ea7e7be1" class="bulleted-list"><li style="list-style-type:circle">paper for later - <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></li></ul><hr id="1dbe3b49-c04d-8091-833a-d101b373d2c1"/><ul id="1dbe3b49-c04d-80af-a4b3-eb68420b5f3e" class="bulleted-list"><li style="list-style-type:circle">Langchain offers a common interface, for these model to embed the documents or just text using  <code>embed_documents</code>, <code>embed_query</code> methods</li></ul><ul id="1dbe3b49-c04d-8073-9a17-d9ddae3c54c0" class="bulleted-list"><li style="list-style-type:circle">Langchain integration supports for different providers - <a href="https://python.langchain.com/docs/integrations/text_embedding/">https://python.langchain.com/docs/integrations/text_embedding/</a></li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/retrievers/">Retrievers</a></summary><div class="indented"><ul id="1dbe3b49-c04d-802c-85e8-e255d62c02d0" class="bulleted-list"><li style="list-style-type:disc">The interface takes in an input query which is a string and returns the list of documents <figure id="1dbe3b49-c04d-80da-ba37-f8be26c6d2b7" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image.png"><img style="width:570px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image.png"/></a></figure></li></ul><ul id="1dbe3b49-c04d-809b-83a2-f861e22faeb2" class="bulleted-list"><li style="list-style-type:disc">This class is a <code>Runnable</code> type, which means the Runnable class methods are available and we can use <code>invoke</code> method<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1dbe3b49-c04d-801a-b8a1-ce3537ea159a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1dbe3b49-c04d-80c3-992c-ee3193e2f734" class=""><a href="https://python.langchain.com/docs/how_to/lcel_cheatsheet/">https://python.langchain.com/docs/how_to/lcel_cheatsheet/</a> </p></div></figure></li></ul><ul id="1dbe3b49-c04d-80be-aa52-dbb5f5c37675" class="bulleted-list"><li style="list-style-type:disc">It requires that the method <code>_get_relevant_documents</code> method is implemented.</li></ul><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1dbe3b49-c04d-8086-8137-e522825bb15d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1dbe3b49-c04d-8030-b5b3-cb923b3e7793" class=""><a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">https://www.pinecone.io/learn/series/langchain/langchain-expression-language/</a><br/>the LECL usage broken down<br/></p><div><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">LECL <a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">LangChain Expression Language</a></summary><div class="indented"><ul id="1dbe3b49-c04d-8026-9716-cc54fef013a8" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">https://www.pinecone.io/learn/series/langchain/langchain-expression-language/</a></li></ul><ul id="1dbe3b49-c04d-8073-9638-c3a55c69e1c4" class="bulleted-list"><li style="list-style-type:disc">Traditional syntax for calling a chain<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-80e1-9312-f6580864b1e9" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">chain = LLMChain(
    prompt=prompt,
    llm=model,
    output_parser=output_parser
)</code></pre></li></ul><ul id="1dbe3b49-c04d-801f-a6d0-e674c8f9b17b" class="bulleted-list"><li style="list-style-type:disc"> This uses pipe, which is | operator, it is the <code>__or__</code> method on the class.<ul id="1dbe3b49-c04d-8057-ba21-e27a3a6f11e0" class="bulleted-list"><li style="list-style-type:circle">It take the output from left operations and passes it as input to the neighboring right operations.</li></ul><ul id="1dbe3b49-c04d-807a-811f-ee2f779b6a2e" class="bulleted-list"><li style="list-style-type:circle">Sample example</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-8039-9259-fa01f337d77b" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">def add_five(x):
    return x + 5

def multiply_by_two(x):
    return x * 2

# wrap the functions with Runnable
add_five = Runnable(add_five)
multiply_by_two = Runnable(multiply_by_two)

# run them using the object approach
chain = add_five.__or__(multiply_by_two)
chain(3)  # should return 16</code></pre></li></ul><ul id="1dbe3b49-c04d-8004-a663-dcb31b63c3fe" class="bulleted-list"><li style="list-style-type:disc">A simple RAG pipeline is created.<ul id="1dbe3b49-c04d-8030-b34c-ec1d0a103ee3" class="bulleted-list"><li style="list-style-type:circle">with 2 retrievals, that have half of the information.</li></ul><ul id="1dbe3b49-c04d-80f0-9a04-dd86b61c3f14" class="bulleted-list"><li style="list-style-type:circle">prompt the model using the first retrieval. (it does not have the data)</li></ul></li></ul><ul id="1dbe3b49-c04d-8079-912d-cb793bdf0baa" class="bulleted-list"><li style="list-style-type:disc">New concepts<figure id="1dbe3b49-c04d-80c9-b924-d7965c0edf45" class="image"><a href="LangChain%201c4e3b49c04d800d8533f6985c38fae5/image.png"><img style="width:3442px" src="LangChain%201c4e3b49c04d800d8533f6985c38fae5/image.png"/></a></figure><ul id="1dbe3b49-c04d-80e1-9717-df8b6ebedf0d" class="bulleted-list"><li style="list-style-type:circle">RunnableParallel<ul id="1dbe3b49-c04d-8085-b308-d4942c5117ce" class="bulleted-list"><li style="list-style-type:square"><code>RunnableParallel</code> object allows us to define multiple values and operations, and run them all in parallel. </li></ul></li></ul><ul id="1dbe3b49-c04d-8025-a290-d0ca3fae7c63" class="bulleted-list"><li style="list-style-type:circle">RunnablePassThrough<ul id="1dbe3b49-c04d-803f-ad20-c7f0f37af4d7" class="bulleted-list"><li style="list-style-type:square">The <code>RunnablePassthrough</code> object is used as a &quot;passthrough&quot; take takes any input to the current component (retrieval) and allows us to provide it in the component output via the &quot;question&quot; key.</li></ul></li></ul></li></ul><ul id="1dbe3b49-c04d-8050-99ae-d149621ff196" class="bulleted-list"><li style="list-style-type:disc">when the chain that is defined using the pipe operator is then invoked. using on vector_a as the context. The llm does not have an answer. which is expected.<ul id="1dbe3b49-c04d-8068-b4c0-c520f03c9063" class="bulleted-list"><li style="list-style-type:circle">Fortunately, <code>Runnable Parallel</code> allows us to provide multiple contexts, and this time we can pass both halves of the information.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-8022-ab5d-e3c1131e8103" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">prompt_str = &quot;&quot;&quot;Answer the question below using the context:

Context:
{context_a}
{context_b}

Question: {question}

Answer: &quot;&quot;&quot;
prompt = ChatPromptTemplate.from_template(prompt_str)

retrieval = RunnableParallel(
    {
        &quot;context_a&quot;: retriever_a, &quot;context_b&quot;: retriever_b,
        &quot;question&quot;: RunnablePassthrough()
    }
)

chain = retrieval | prompt | model | output_parser</code></pre></li></ul><ul id="1dbe3b49-c04d-802f-94b3-e1a6504001a6" class="bulleted-list"><li style="list-style-type:disc">The <code>RunnableLambda</code> is a LangChain abstraction that allows us to turn Python functions into pipe-compatible functions,<ul id="1dbe3b49-c04d-808e-8ca4-d4d7feeb1b2a" class="bulleted-list"><li style="list-style-type:circle">THE PIPE OPERATOR CAN BE USED WHEN A FUNCTION IS CONVERTED INTO <code>RUNNABLELAMBDA</code></li></ul></li></ul><p id="1dbe3b49-c04d-8096-9645-e1a5dfa3ba71" class="">
</p><p id="1dbe3b49-c04d-801b-bd95-fe65941d2dfb" class="">
</p></div></details></div></div></figure><ul id="1dbe3b49-c04d-8028-8de5-cfd3fe837401" class="bulleted-list"><li style="list-style-type:disc">Retrievers do not store documents</li></ul><ul id="1dbe3b49-c04d-80aa-ba96-d7b9cf460cda" class="bulleted-list"><li style="list-style-type:disc">Common types of retrieval systems<ul id="1dbe3b49-c04d-8071-ac50-d464f4d15c1a" class="bulleted-list"><li style="list-style-type:circle">APIs</li></ul><ul id="1dbe3b49-c04d-8051-baaa-f705ff80b5b2" class="bulleted-list"><li style="list-style-type:circle">RDBMS<ul id="1dbe3b49-c04d-80e7-9b30-f7c4eb8889b4" class="bulleted-list"><li style="list-style-type:square">In these cases, <a href="https://python.langchain.com/docs/concepts/retrieval/">query analysis</a> techniques to construct a structured query from natural language is critical.</li></ul><ul id="1dbe3b49-c04d-80b5-bb45-d3c3713a97a5" class="bulleted-list"><li style="list-style-type:square">For example, you can build a retriever for a SQL database using <br/>text-to-SQL conversion. This allows a natural language query (string) <br/>retriever to be transformed into a SQL query behind the scenes.<br/></li></ul></li></ul><ul id="1dbe3b49-c04d-8064-9060-f25eb8d165b1" class="bulleted-list"><li style="list-style-type:circle">Lexical Search<ul id="1dbe3b49-c04d-80c7-83d8-ee27e403aa5b" class="bulleted-list"><li style="list-style-type:square">Many search engine use lexical search, words in the query to match with words in the documents</li></ul><ul id="1dbe3b49-c04d-8076-b64e-f714f983aeb3" class="bulleted-list"><li style="list-style-type:square">Example: TF-IDF, or BM-25</li></ul><ul id="1dbe3b49-c04d-80fe-a2cf-f406590ff574" class="bulleted-list"><li style="list-style-type:square">There’ elastic search integration available for langchain</li></ul></li></ul><ul id="1dbe3b49-c04d-80bf-86b8-eacfbdbb57ca" class="bulleted-list"><li style="list-style-type:circle"> Vector Store<ul id="1dbe3b49-c04d-8078-973f-ea80406e1388" class="bulleted-list"><li style="list-style-type:square">Powerful when dealing with unstructured data</li></ul><ul id="1dbe3b49-c04d-8062-b866-d958ec7eac08" class="bulleted-list"><li style="list-style-type:square">You can convert a vector store in a retrieval using the <code>as_retriever()</code> method</li></ul></li></ul></li></ul><ul id="1dbe3b49-c04d-8070-897b-d442fe1540c0" class="bulleted-list"><li style="list-style-type:disc">Advanced search techniques<ul id="1dbe3b49-c04d-8021-8595-dcbb6e6f6804" class="bulleted-list"><li style="list-style-type:circle">Ensemble<ul id="1dbe3b49-c04d-80a6-ad14-db515f8d0b6e" class="bulleted-list"><li style="list-style-type:square">It involves combines results of different retrievals that are good at finding different aspects of the query with the documents.</li></ul><ul id="1dbe3b49-c04d-8067-a789-e9aee01d566e" class="bulleted-list"><li style="list-style-type:square">pass the retrievers in the list as input, specify the weights for each one.</li></ul><ul id="1dbe3b49-c04d-80fd-b2c1-cc7955ebd4e5" class="bulleted-list"><li style="list-style-type:square">Finally, reranking is done, using another algorithm</li></ul></li></ul><ul id="1dbe3b49-c04d-8062-8806-f025329fd505" class="bulleted-list"><li style="list-style-type:circle">Source Document retention<ul id="1dbe3b49-c04d-80c0-acdc-ffdcf8080401" class="bulleted-list"><li style="list-style-type:square">The main idea is — after the indexing is done, being able to trace back to the original document .</li></ul><ul id="1dbe3b49-c04d-806d-870a-e6698c6b63b9" class="bulleted-list"><li style="list-style-type:square">2 types<ul id="1dbe3b49-c04d-8006-a824-daf6766b9634" class="bulleted-list"><li style="list-style-type:disc">ParentDocument <ul id="1dbe3b49-c04d-804f-96ee-f988871211e4" class="bulleted-list"><li style="list-style-type:circle">retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.</li></ul></li></ul><ul id="1dbe3b49-c04d-80b2-9888-e618945d07f3" class="bulleted-list"><li style="list-style-type:disc">MultiVector<ul id="1dbe3b49-c04d-8048-9b00-dfd378dd0581" class="bulleted-list"><li style="list-style-type:circle">This may use LLM to store different transformation of the source document, perhaps captures things that provide various aspects of the source, using hypothetical questions or it can just be summary.</li></ul></li></ul></li></ul></li></ul></li></ul><p id="1dbe3b49-c04d-805e-a3c7-dc9603582217" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/vectorstores/">Vector stores</a></summary><div class="indented"><ul id="1dbe3b49-c04d-808b-8862-c5d81b1b9b07" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/embedding_models/">Embeddings</a></li></ul><ul id="1dbe3b49-c04d-80b1-902a-e449d8a52571" class="bulleted-list"><li style="list-style-type:disc">These are used to index documents(unstructured) into vector representation ( called embeddings) and then use for efficient retrieval for a given query.<figure id="1dbe3b49-c04d-80bc-913b-eb21f925f70c" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%201.png"><img style="width:570.0064697265625px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%201.png"/></a></figure></li></ul><p id="1dbe3b49-c04d-80ef-bc47-d25e8f0938c3" class="">
</p><ul id="1dbe3b49-c04d-80d3-a4b4-fe94ac86452a" class="bulleted-list"><li style="list-style-type:disc">LangChain provides a standard interface for working with vector <br/>stores, allowing users to easily switch between different vectorstore <br/>implementations.<br/></li></ul><ul id="1dbe3b49-c04d-80b6-9b83-f34aa303f3c5" class="bulleted-list"><li style="list-style-type:disc">The interface consists of basic methods for writing, deleting and searching for documents in the vector store.</li></ul><ul id="1dbe3b49-c04d-80f5-8d1d-fc5cf0eea74d" class="bulleted-list"><li style="list-style-type:disc">The key methods are:<ul id="1dbe3b49-c04d-8053-b7b3-ebf41992859d" class="bulleted-list"><li style="list-style-type:circle"><code>add_documents</code>: Add a list of texts to the vector store.</li></ul><ul id="1dbe3b49-c04d-8033-b196-d3faac4db19b" class="bulleted-list"><li style="list-style-type:circle"><code>delete</code>: Delete a list of documents from the vector store.</li></ul><ul id="1dbe3b49-c04d-8069-a4d4-c7f9b51822c6" class="bulleted-list"><li style="list-style-type:circle"><code>similarity_search</code>: Search for similar documents to a given query.</li></ul></li></ul><ul id="1dbe3b49-c04d-8038-9c1f-cfa3b376fc73" class="bulleted-list"><li style="list-style-type:disc">Provide the document ids when adding or deleting documents, as it makes the operations efficient</li></ul><ul id="1dbe3b49-c04d-803f-afdd-f58ffd0b05ce" class="bulleted-list"><li style="list-style-type:disc">Similarity Search<ul id="1dbe3b49-c04d-8061-81f5-ef3f911fe869" class="bulleted-list"><li style="list-style-type:circle">Now that documents are represented as some Vectors. Given a query in text format, is it converted into an embedding and a similarity check algorithm is run to score the matches and then rank the results.</li></ul><ul id="1dbe3b49-c04d-80d9-ac6f-d4cefcf8e6c8" class="bulleted-list"><li style="list-style-type:circle">Score or Metrics<ul id="1dbe3b49-c04d-8035-8022-f17cf1c74a3b" class="bulleted-list"><li style="list-style-type:square">Cosine</li></ul><ul id="1dbe3b49-c04d-805f-8890-e647f6d3e08e" class="bulleted-list"><li style="list-style-type:square">Dot product</li></ul><ul id="1dbe3b49-c04d-803a-9952-ff9a50cf5747" class="bulleted-list"><li style="list-style-type:square">Euclidean distance</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-80e0-9c64-fb2a069d4379" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">## example code snippets

from langchain_core.vectorstores import InMemoryVectorStore
# Initialize with an embedding model
vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())

from langchain_core.documents import Document

document_1 = Document(
    page_content=&quot;I had chocalate chip pancakes and scrambled eggs for breakfast this morning.&quot;,
    metadata={&quot;source&quot;: &quot;tweet&quot;},
)

document_2 = Document(
    page_content=&quot;The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.&quot;,
    metadata={&quot;source&quot;: &quot;news&quot;},
)

documents = [document_1, document_2]

vector_store.add_documents(documents=documents)

vector_store.add_documents(documents=documents, ids=[&quot;doc1&quot;, &quot;doc2&quot;])
vector_store.delete(ids=[&quot;doc1&quot;])

query = &quot;my query&quot;
docs = vectorstore.similarity_search(query)</code></pre></li></ul><p id="1dbe3b49-c04d-8035-bd38-eed0a8e86317" class="">
</p><ul id="1dbe3b49-c04d-807f-b370-e4487a926017" class="bulleted-list"><li style="list-style-type:disc">MetaData filtering<ul id="1dbe3b49-c04d-8057-9ef9-f7f0698cccd1" class="bulleted-list"><li style="list-style-type:circle">the interface offers filtering based on properties available in the documents. These are provided during creation of documents, as the metadata argument</li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a></summary><div class="indented"><ul id="1dbe3b49-c04d-80ab-a990-f1e5c2b638d5" class="bulleted-list"><li style="list-style-type:disc">Splitting is done for the following reasons<ul id="1dbe3b49-c04d-80b6-b2e3-dcaf545531e7" class="bulleted-list"><li style="list-style-type:circle">Limited windows size of the model</li></ul><ul id="1dbe3b49-c04d-8052-9876-cecc60232ae3" class="bulleted-list"><li style="list-style-type:circle">consistent length of documents</li></ul><ul id="1dbe3b49-c04d-806e-ae21-c4a6a3932b23" class="bulleted-list"><li style="list-style-type:circle">optimizing training performance</li></ul><ul id="1dbe3b49-c04d-806e-8af6-d92a38116820" class="bulleted-list"><li style="list-style-type:circle">improves the quality of vectors/representation </li></ul></li></ul><ul id="1dbe3b49-c04d-8060-a610-e0be8cf35602" class="bulleted-list"><li style="list-style-type:disc">Types<ul id="1dbe3b49-c04d-803b-a496-ca2c6c7686c9" class="bulleted-list"><li style="list-style-type:circle">Length<ul id="1dbe3b49-c04d-8086-86c1-fc2c77f7cfe8" class="bulleted-list"><li style="list-style-type:square">Number of characters, or tokens</li></ul></li></ul><ul id="1dbe3b49-c04d-806a-a4b9-de28d18dd248" class="bulleted-list"><li style="list-style-type:circle">Text <ul id="1dbe3b49-c04d-801e-aceb-fe2f8f4c888d" class="bulleted-list"><li style="list-style-type:square">It tries to keep related information together, like a paragraph, sentence, words. </li></ul></li></ul><ul id="1dbe3b49-c04d-8053-bb65-d7c023d73432" class="bulleted-list"><li style="list-style-type:circle">Document<ul id="1dbe3b49-c04d-80d6-81bb-cbeaaa1cfa1f" class="bulleted-list"><li style="list-style-type:square">If it;s a markdown, then split based on the different tags it uses.</li></ul><ul id="1dbe3b49-c04d-809a-b895-d1c54d44b672" class="bulleted-list"><li style="list-style-type:square">JSON, HTML, </li></ul></li></ul><ul id="1dbe3b49-c04d-808a-bbab-e6b6b67d1578" class="bulleted-list"><li style="list-style-type:circle">Semantic<ul id="1dbe3b49-c04d-8056-a7db-eabf14d612f1" class="bulleted-list"><li style="list-style-type:square">While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text&#x27;s semantics. </li></ul></li></ul></li></ul></div></details></li></ul><p id="1dbe3b49-c04d-80db-adc1-f90044583e33" class="">
</p><h3 id="1dbe3b49-c04d-806e-8027-df2494029aa3" class="">Semantic Search</h3><ul id="1dbe3b49-c04d-802f-83b1-c7770de98bc1" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/tutorials/retrievers/">https://python.langchain.com/docs/tutorials/retrievers/</a></li></ul><ul id="1dbe3b49-c04d-8098-a8a0-e9b0546122ba" class="bulleted-list"><li style="list-style-type:disc">This whole thing can be broken into the following steps<ol type="1" id="1dbe3b49-c04d-80f4-8d49-e7ab7b3f582b" class="numbered-list" start="1"><li>loading<ol type="a" id="1dbe3b49-c04d-80ae-b6e3-f034252e9818" class="numbered-list" start="1"><li>use document loaders</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8000-ba34-c01f03fb510e" class="numbered-list" start="2"><li>breaking into chunks<ol type="a" id="1dbe3b49-c04d-801f-8da4-dbed24279ca4" class="numbered-list" start="1"><li>some sort of splitters</li></ol><ol type="a" id="1dbe3b49-c04d-80f4-b615-def15309e801" class="numbered-list" start="2"><li>to ensure it fits into the model’s finite context windows size</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8027-8b30-e173c033e702" class="numbered-list" start="3"><li>Storing into vector dbs<figure id="1dbe3b49-c04d-80b8-b762-e9ea96c11f34" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%202.png"><img style="width:2583px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%202.png"/></a></figure></li></ol><ol type="1" id="1dbe3b49-c04d-80cb-b10b-e906ecb99c38" class="numbered-list" start="4"><li>retrieval:<ol type="a" id="1dbe3b49-c04d-801b-9538-cb0b483bca0a" class="numbered-list" start="1"><li>given a user input, retrieves the relevant splits</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8076-b8fc-f8220efa4815" class="numbered-list" start="5"><li>generate<ol type="a" id="1dbe3b49-c04d-8067-866c-d75783ce9405" class="numbered-list" start="1"><li>user prompt, along with the document as the input to the models to generate an answer</li></ol><figure id="1dbe3b49-c04d-8081-a71a-d216ae89a09e" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%203.png"><img style="width:2532px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%203.png"/></a></figure></li></ol></li></ul><ul id="1dbe3b49-c04d-8064-81ff-e107b8787b3a" class="bulleted-list"><li style="list-style-type:disc">Once we&#x27;ve indexed our data, we will use <a href="https://langchain-ai.github.io/langgraph/">LangGraph</a> as our orchestration framework to implement the retrieval and generation steps.</li></ul></div></details><p id="1dce3b49-c04d-8082-b7ec-f8ecd9b9bc42" class="">
</p><h1 id="1dce3b49-c04d-8026-8bc4-ea3d4105129e" class="">Coding Part 1</h1><ul id="1dce3b49-c04d-80b6-919e-da3f10dcd98c" class="bulleted-list"><li style="list-style-type:disc">After getting the langsmith setup ready<ul id="1dce3b49-c04d-80df-ad8a-f75e332984e3" class="bulleted-list"><li style="list-style-type:circle">install packages</li></ul><ul id="1dce3b49-c04d-8073-b683-cfc3499e0139" class="bulleted-list"><li style="list-style-type:circle">Store the keys</li></ul></li></ul><ul id="1dce3b49-c04d-80ba-91dd-d90be5a0715a" class="bulleted-list"><li style="list-style-type:disc">Now, I require 3 components<ul id="1dce3b49-c04d-806a-90d7-dcac7ae71ec0" class="bulleted-list"><li style="list-style-type:circle">Model</li></ul><ul id="1dce3b49-c04d-80c1-b40a-d6e4686d113c" class="bulleted-list"><li style="list-style-type:circle">Embedding</li></ul><ul id="1dce3b49-c04d-800c-97a4-d07558a5d08c" class="bulleted-list"><li style="list-style-type:circle">Vectorstore</li></ul></li></ul><ul id="1dce3b49-c04d-80c0-827c-daad632bd3d1" class="bulleted-list"><li style="list-style-type:disc">The corresponding commands are<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dce3b49-c04d-8093-aebe-e9c5f7589e33" class="code"><code class="language-JavaScript">pip install -qU &quot;langchain[openai]&quot;
pip install -U langchain langchain-openai
pip install -qU langchain-core</code></pre></li></ul><p id="1dce3b49-c04d-8020-8439-e698cadfcfde" class="">
</p><h3 id="1dce3b49-c04d-80ec-a83b-c42b08fecf5f" class="">GOAL</h3><ul id="1dce3b49-c04d-80d7-a634-cfd6dd63c7c6" class="bulleted-list"><li style="list-style-type:disc">build an app that answers questions about the website&#x27;s content. The specific website we will use is the <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a> blog post by Lilian Weng, which allows us to ask questions about the contents of the post.</li></ul><p id="1dce3b49-c04d-807e-b7b7-d1de1d8be10c" class="">
</p><ul id="1dce3b49-c04d-8022-bba7-c101e81082a0" class="bulleted-list"><li style="list-style-type:disc">Update the LLM init code to use the langsmith</li></ul><ul id="1dce3b49-c04d-80fb-9aae-c78802989921" class="bulleted-list"><li style="list-style-type:disc">Trace output</li></ul><ul id="1e1e3b49-c04d-80bb-beda-d624022c9b34" class="bulleted-list"><li style="list-style-type:disc"><a href="https://smith.langchain.com/">https://smith.langchain.com/</a><figure id="1dce3b49-c04d-80dc-b6c1-f5b0d66f6b42" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%204.png"><img style="width:2866px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%204.png"/></a></figure></li></ul><ul id="1dce3b49-c04d-8085-8c9a-c362a55375b8" class="bulleted-list"><li style="list-style-type:disc">Relation between Embedding and Model<ul id="1e1e3b49-c04d-80fb-93fc-d144a208e3b8" class="bulleted-list"><li style="list-style-type:circle">In essence: The embedding model provides the &quot;language&quot; for the RAG system to understand and compare information, while the LLM (and potentially rerankers) handle <br/>the retrieval and generation aspects.<br/></li></ul></li></ul><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Purpose of some code blocks</summary><div class="indented"><h2 id="1e1e3b49-c04d-8005-8b3f-f8fa8f7526a2" class="">Indexing</h2><ul id="1e1e3b49-c04d-800f-8308-f3d42e13f45f" class="bulleted-list"><li style="list-style-type:disc">In this case, WebBaseLoader, is used to load a webpage, and we use soup to focus on relavent tags such as <strong>“post-content”, “post-title”, or “post-header”</strong><blockquote id="1e1e3b49-c04d-8082-917e-f9553c60ad26" class=""><strong>In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs).</strong></blockquote></li></ul><ul id="1e1e3b49-c04d-8028-bc28-e05425183809" class="bulleted-list"><li style="list-style-type:disc">The splits are to be converted into embeddings, this is done by added them to the vector store<ul id="1e1e3b49-c04d-806c-b801-ca8c495ae70d" class="bulleted-list"><li style="list-style-type:circle">in this case, InMemoryDataStore is being used</li></ul><ul id="1e1e3b49-c04d-80b0-a38c-f769189c4dfb" class="bulleted-list"><li style="list-style-type:circle">along with OpenAI embedding<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1e3b49-c04d-80d3-bd69-e0e0bd172e65" class="code"><code class="language-JavaScript">embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-large&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1e3b49-c04d-808e-b340-e1162b8de96a" class="code"><code class="language-JavaScript">vector_store.add_documents(documents=all_splits)</code></pre></li></ul></li></ul><ul id="1e1e3b49-c04d-80d9-af4b-c0b5c79f022a" class="bulleted-list"><li style="list-style-type:disc">At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.<hr id="1e1e3b49-c04d-80e6-9f87-c0753890e5ae"/></li></ul><ul id="1dce3b49-c04d-80c4-b7fc-f214edcffab6" class="bulleted-list"><li style="list-style-type:disc">About the prompt using hub.pull<ul id="1e1e3b49-c04d-8006-96c2-ce0ab59605bc" class="bulleted-list"><li style="list-style-type:circle"><a href="https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub">https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub</a><ul id="1e1e3b49-c04d-8096-a6fc-cf7cfd47b37d" class="bulleted-list"><li style="list-style-type:square">available prompts that can be pulled to chat with the LLM.</li></ul><ul id="1e1e3b49-c04d-8088-81b6-f05185defcc0" class="bulleted-list"><li style="list-style-type:square">Depending on the usecase, different prompts are available. <ul id="1e1e3b49-c04d-80e4-be3b-e9c7fbb5b51b" class="bulleted-list"><li style="list-style-type:disc">Since, we are trying to build rag, using <a href="https://smith.langchain.com/hub/rlm/rag-prompt">https://smith.langchain.com/hub/rlm/rag-prompt</a>, to predefine things for us</li></ul><ul id="1e1e3b49-c04d-8098-ab4a-d546b66f7252" class="bulleted-list"><li style="list-style-type:disc">it defines the question, context that are going to be passed in the prompt, and the expected output from the LLM</li></ul></li></ul></li></ul><ul id="1e1e3b49-c04d-8094-806f-ecf7dcd683db" class="bulleted-list"><li style="list-style-type:circle"><code>StateGraph</code> is part of the langchain graph module. It helps using a State, which is essentially a dictionary, with the keys being the variable/placeholders that are being used by the LLM (originating from the prompt that’s defined)<ul id="1e1e3b49-c04d-8065-b90c-e974d9bfb993" class="bulleted-list"><li style="list-style-type:square">defines the order of operations</li></ul><ul id="1e1e3b49-c04d-8071-baf0-d2fd5fbc4d67" class="bulleted-list"><li style="list-style-type:square">and add edges</li></ul><ul id="1e1e3b49-c04d-80ac-aba6-c475535467c1" class="bulleted-list"><li style="list-style-type:square">compile, so that it becomes a Runnable object</li></ul></li></ul></li></ul><h2 id="1e1e3b49-c04d-8002-a5b3-f0f76e9e0b73" class="">Retrieval and Generation</h2><ul id="1e1e3b49-c04d-801c-b925-df378bfa54c5" class="bulleted-list"><li style="list-style-type:disc">To use LangGraph, we need to define three things:<ol type="1" id="1e1e3b49-c04d-8072-b3a2-dee333ed899e" class="numbered-list" start="1"><li>The state of our application;</li></ol><ol type="1" id="1e1e3b49-c04d-800f-840d-ecdd2d7ee82e" class="numbered-list" start="2"><li>The nodes of our application (i.e., application steps);</li></ol><ol type="1" id="1e1e3b49-c04d-8036-95bf-d1ac640b751f" class="numbered-list" start="3"><li>The &quot;control flow&quot; of our application (e.g., the ordering of the steps).</li></ol><h3 id="1e1e3b49-c04d-80e6-81d7-db31c14275ed" class="">State: </h3><ul id="1e1e3b49-c04d-8079-8a29-c9cdc6375580" class="bulleted-list"><li style="list-style-type:circle">of our application controls what data is input to the application, transferred between steps, and output by the application. </li></ul><ul id="1e1e3b49-c04d-80b9-88d9-c0958fb25497" class="bulleted-list"><li style="list-style-type:circle">It is typically a <code>TypedDict</code>, but can also be a <a href="https://langchain-ai.github.io/langgraph/how-tos/state-model/">Pydantic BaseModel</a></li></ul></li></ul><p id="1e1e3b49-c04d-8094-ba2a-c97b9aa8087c" class="">
</p><h2 id="1e1e3b49-c04d-8022-9070-dfffed417b16" class="">Query Analysis</h2><ul id="1e1e3b49-c04d-8024-855d-f468d161fe20" class="bulleted-list"><li style="list-style-type:disc">The state can be extended to define the output structure using a pydantic model or typedDict</li></ul><ul id="1e1e3b49-c04d-80f6-87f1-cb69a83f1711" class="bulleted-list"><li style="list-style-type:disc">If pydantic model is used, the return type will be a runnable object else a json string format.</li></ul><ul id="1e1e3b49-c04d-803b-9a0b-e7922959eafa" class="bulleted-list"><li style="list-style-type:disc">Add it as a step before the retrival</li></ul><ul id="1e1e3b49-c04d-804f-af3f-d68dcde5c4f9" class="bulleted-list"><li style="list-style-type:disc">- <a href="https://python.langchain.com/docs/how_to/structured_output/">https://python.langchain.com/docs/how_to/structured_output/</a></li></ul><ul id="1e1e3b49-c04d-80d8-a220-c7ebe4ee250c" class="bulleted-list"><li style="list-style-type:disc">Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. F</li></ul><ul id="1e1e3b49-c04d-80da-9758-ef2be84d2947" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/how_to/#query-analysis">https://python.langchain.com/docs/how_to/#query-analysis</a></li></ul><p id="1e1e3b49-c04d-800d-ac8a-cfe59229a02f" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Part 2 </summary><div class="indented"><ul id="1e1e3b49-c04d-8032-8193-efa05eecf3f7" class="bulleted-list"><li style="list-style-type:disc">Recap: Part 1 was about RAG and a minimal implementation.</li></ul><ul id="1e1e3b49-c04d-806c-95c4-fcddebb934e4" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/tutorials/qa_chat_history/">Part 2</a> extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.</li></ul><ul id="1e1e3b49-c04d-8070-826a-c4e371e85969" class="bulleted-list"><li style="list-style-type:disc">So far, what has been built is <ul id="1e1e3b49-c04d-80f0-91d1-c02c862cd70d" class="bulleted-list"><li style="list-style-type:circle">loading documents</li></ul><ul id="1e1e3b49-c04d-808a-8046-d4c7c5f0e578" class="bulleted-list"><li style="list-style-type:circle">splitting into chunks </li></ul><ul id="1e1e3b49-c04d-80f8-851e-e909b849bbb4" class="bulleted-list"><li style="list-style-type:circle">indexing <ul id="1e1e3b49-c04d-8060-a5f4-d64e26be2abf" class="bulleted-list"><li style="list-style-type:square">use some embedding</li></ul><ul id="1e1e3b49-c04d-8028-94a4-d2231b2a9d84" class="bulleted-list"><li style="list-style-type:square">convert the chunks into vectors </li></ul></li></ul><ul id="1e1e3b49-c04d-80d7-84d0-fbe1c9392b4d" class="bulleted-list"><li style="list-style-type:circle">store the embedding on chunks into vector store</li></ul><ul id="1e1e3b49-c04d-80e3-a5ef-c686022c5438" class="bulleted-list"><li style="list-style-type:circle">Create a state to capture the placeholders which uses dict to capture results during various steps of the process<ul id="1e1e3b49-c04d-800f-99bb-df13a04a1937" class="bulleted-list"><li style="list-style-type:square">Use stategraph to list the steps<ul id="1e1e3b49-c04d-8078-a437-c977a21100e3" class="bulleted-list"><li style="list-style-type:disc">retrieval : to fetch the relavant documents using some similarity search with the given query</li></ul><ul id="1e1e3b49-c04d-8074-a77f-f42890935b62" class="bulleted-list"><li style="list-style-type:disc">generation: invokes the llm</li></ul></li></ul><ul id="1e1e3b49-c04d-80f8-a49c-cceabaa78335" class="bulleted-list"><li style="list-style-type:square">Add retrieval as the first node in the graph</li></ul><ul id="1e1e3b49-c04d-8026-ba7b-de25ca4dca0c" class="bulleted-list"><li style="list-style-type:square">compile it</li></ul></li></ul><ul id="1e1e3b49-c04d-809d-9a53-fd7bb2e317b5" class="bulleted-list"><li style="list-style-type:circle">Side note<ul id="1e1e3b49-c04d-8085-9ced-cc2ac1eacf64" class="bulleted-list"><li style="list-style-type:square">the state can be extended to define structured output, and this can be added to the state, and as the first step in the stategraph</li></ul></li></ul><hr id="1e1e3b49-c04d-80c7-a1a1-c379e93cc75f"/><p id="1e1e3b49-c04d-8067-894a-ca4e9e7c0f89" class="">
</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1e1e3b49-c04d-80c6-941e-df8c14674d09"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1e1e3b49-c04d-8080-addd-cad28d984af8" class="">Reading material</p><ul id="1e1e3b49-c04d-8063-a35f-fcee7ffa68ae" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/how_to/qa_sources/">Return sources</a>: Learn how to return source documents</li></ul><ul id="1e1e3b49-c04d-8030-866f-d62d1960241f" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/how_to/streaming/">Streaming</a>: Learn how to stream outputs and intermediate steps</li></ul><ul id="1e1e3b49-c04d-80fd-8127-e2ba5473d351" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/how_to/message_history/">Add chat history</a>: Learn how to add chat history to your app</li></ul><ul id="1e1e3b49-c04d-80a2-bde1-d90e322c5dbf" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/retrieval/">Retrieval conceptual guide</a>: A high-level overview of specific retrieval techniques</li></ul></div></figure><p id="1e1e3b49-c04d-80b1-902a-cdaa9a167939" class="">
</p><p id="1e1e3b49-c04d-80ea-87ec-db6cb9d53505" class="">
</p><hr id="1e1e3b49-c04d-8021-a949-c4b96b56756d"/><p id="1e1e3b49-c04d-803a-af40-c2d463d40280" class="">
</p><p id="1e1e3b49-c04d-80a2-b472-c1f074dd49fd" class="">
</p><ul id="1e1e3b49-c04d-805c-a57d-cf1e061e1a34" class="bulleted-list"><li style="list-style-type:circle">There are 2 approaches <ul id="1e1e3b49-c04d-8095-a513-fa5c203aabf6" class="bulleted-list"><li style="list-style-type:square">Chains</li></ul><ul id="1e1e3b49-c04d-8076-8807-ebeaadfbcdf0" class="bulleted-list"><li style="list-style-type:square">Agents</li></ul></li></ul><p id="1e1e3b49-c04d-8066-b721-effdec9e1d87" class="">
</p><ol type="1" id="1e1e3b49-c04d-8043-a4bd-f828e22659d5" class="numbered-list" start="1"><li>Chains<ul id="1e1e3b49-c04d-8070-b7e7-e77e071f8539" class="bulleted-list"><li style="list-style-type:disc">In the <a href="https://python.langchain.com/docs/tutorials/rag/">Part 1</a> of the RAG tutorial, we represented the user input, retrieved context, <br/>and generated answer as separate keys in the state. Conversational <br/>experiences can be naturally represented using a sequence of <br/><a href="https://python.langchain.com/docs/concepts/messages/">messages</a>.<br/> In addition to messages from the user and assistant, retrieved <br/>documents and other artifacts can be incorporated into a message <br/>sequence via <br/><a href="https://python.langchain.com/docs/concepts/messages/#toolmessage">tool messages</a>. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have<ol type="1" id="1e1e3b49-c04d-8011-a374-c65dd2783372" class="numbered-list" start="1"><li>User input as a <code>HumanMessage</code>;</li></ol><ol type="1" id="1e1e3b49-c04d-80ac-9657-fcc8f2609d8f" class="numbered-list" start="2"><li>Vector store query as an <code>AIMessage</code> with tool calls;</li></ol><ol type="1" id="1e1e3b49-c04d-8035-8455-dc398f5cc805" class="numbered-list" start="3"><li>Retrieved documents as a <code>ToolMessage</code>;</li></ol><ol type="1" id="1e1e3b49-c04d-80d0-b327-e470f068a5e3" class="numbered-list" start="4"><li>Final response as a <code>AIMessage</code>.</li></ol></li></ul><ul id="1e1e3b49-c04d-8049-addc-f2129f8af459" class="bulleted-list"><li style="list-style-type:disc">This format is common and is availble within langchain </li></ul><ul id="1e1e3b49-c04d-80ca-9234-e575369ab508" class="bulleted-list"><li style="list-style-type:disc">The graph is essentially modified so that, <ul id="1e1e3b49-c04d-807e-b055-c49dbc668fee" class="bulleted-list"><li style="list-style-type:circle">the query are rewritten by the model to provide more context. it uses memory or chat history.</li></ul><ul id="1e1e3b49-c04d-8072-a410-fb7b5006c58c" class="bulleted-list"><li style="list-style-type:circle">for example: a prompt may refer to “it”, the tooling allows the LLM to rewrite the query to specify that “it” means “tool decomposition” which enables for better responses</li></ul><p id="1e1e3b49-c04d-80e0-a589-f5f2a55ab9ec" class="">
</p></li></ul><hr id="1e1e3b49-c04d-80a9-bc6c-ea89269e4204"/><h2 id="1e1e3b49-c04d-8004-aac2-ea562616b6e9" class="">Tooling</h2><figure id="1e1e3b49-c04d-80dd-8393-edbbd36d44f4" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%205.png"><img style="width:626px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%205.png"/></a></figure><ul id="1e1e3b49-c04d-809c-a5e6-c35ba6e68f95" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/tool_calling/">https://python.langchain.com/docs/concepts/tool_calling/</a></li></ul><ul id="1e1e3b49-c04d-802d-9372-d781448c2185" class="bulleted-list"><li style="list-style-type:disc">Generally, an LLM responds to user prompts directly. (meaning it uses the documents used during training or the ones provided as part of the rag pipeline).<ul id="1e1e3b49-c04d-8014-bbcb-fb277405fc40" class="bulleted-list"><li style="list-style-type:circle">But we also need it interact with external systems such as APIs, and format the response in a way that LLM can consume it</li></ul></li></ul><ul id="1e1e3b49-c04d-80b0-8e1e-e7319baebdfe" class="bulleted-list"><li style="list-style-type:disc">utilizing Tooling<ol type="1" id="1e1e3b49-c04d-8023-b212-c3067d09a37b" class="numbered-list" start="1"><li>Tool creation<ol type="a" id="1e1e3b49-c04d-8035-a0d0-d69d9535c370" class="numbered-list" start="1"><li>is done using the <code>@tool</code> annotation</li></ol></li></ol><ol type="1" id="1e1e3b49-c04d-80ec-97ca-cfc57a53f410" class="numbered-list" start="2"><li>Model awareness or Tool binding<ol type="a" id="1e1e3b49-c04d-8004-825c-d2b2379b53af" class="numbered-list" start="1"><li>Specify the expected schema by the tool </li></ol><ol type="a" id="1e1e3b49-c04d-80de-abc2-d5413570ca6b" class="numbered-list" start="2"><li>Associate the tool with the model, so that the model is now aware that a tool is available</li></ol></li></ol><ol type="1" id="1e1e3b49-c04d-80a4-adaa-c9e60ce1b7f7" class="numbered-list" start="3"><li>Tool Calling<ol type="a" id="1e1e3b49-c04d-80bf-893b-e2e3242a94e4" class="numbered-list" start="1"><li>Model can choose to call and specify the arguments, usually from the context of the conversation</li></ol></li></ol><ol type="1" id="1e1e3b49-c04d-80ce-9fbe-ff502130be31" class="numbered-list" start="4"><li>Tool Execution<ol type="a" id="1e1e3b49-c04d-807f-9a60-c0e209ceb161" class="numbered-list" start="1"><li>could be an API request via HTTP.</li></ol><ol type="a" id="1e1e3b49-c04d-8062-ab11-de396b3aebe7" class="numbered-list" start="2"><li>invocation</li></ol></li></ol><p id="1e1e3b49-c04d-806a-a077-d90dbe7ebe0a" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1e3b49-c04d-80e4-9cbf-ea88ef0de014" class="code"><code class="language-JavaScript"># Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
# Tool calling 
response = model_with_tools.invoke(user_input)


from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot;
    return a * b</code></pre></li></ul><p id="1e1e3b49-c04d-806a-9e2d-f05d2ca7a311" class="">
</p><ul id="1e1e3b49-c04d-80d0-8347-ce57f5436012" class="bulleted-list"><li style="list-style-type:disc">Adding another tool to a model that supports tooling<figure id="1e1e3b49-c04d-8083-817e-e9b4987d2097" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%206.png"><img style="width:598px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%206.png"/></a></figure><ul id="1e1e3b49-c04d-80f0-b35f-ef173e1d1086" class="bulleted-list"><li style="list-style-type:circle">defines the function with <code>@tool</code> and then bind it and use the new reference of the model to invoke with a prompt</li></ul></li></ul><p id="1e1e3b49-c04d-80d3-835c-d15905225cbc" class="">
</p><ul id="1e1e3b49-c04d-8027-9792-e816a9f78d40" class="bulleted-list"><li style="list-style-type:disc">Responses after the invocation is complete.<ul id="1e1e3b49-c04d-80a7-9a89-f5774bfbf055" class="bulleted-list"><li style="list-style-type:circle">The output result will be an <code>AIMessage</code>. But, if the tool was called, result will have a <code>tool_calls</code> attribute. This attribute includes everything needed to execute the tool, including the tool name and input arguments:</li></ul><ul id="1e1e3b49-c04d-802f-b456-cce8a86efe3a" class="bulleted-list"><li style="list-style-type:circle">result would be an <code>AIMessage</code> containing the model&#x27;s response in natural language (e.g., &quot;Hello!&quot;).</li></ul></li></ul><h2 id="1e1e3b49-c04d-809d-bd8b-fdb941ca7a2e" class="">TOOL EXECUTION</h2><ul id="1e1e3b49-c04d-80a4-9bbf-ede300ac8423" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/tools/">Tools</a> implement the <a href="https://python.langchain.com/docs/concepts/runnables/">Runnable</a> interface, which means that they can be invoked (e.g., <code>tool.invoke(args)</code>) directly.</li></ul><ul id="1e1e3b49-c04d-8072-ba86-da9e864446e6" class="bulleted-list"><li style="list-style-type:disc"><a href="https://langchain-ai.github.io/langgraph/">LangGraph</a> offers pre-built components (e.g., <code><a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode">ToolNode</a></code>) that will often invoke the tool in behalf of the user.</li></ul><ul id="1e2e3b49-c04d-809b-a0a2-eaf3e9023f9b" class="bulleted-list"><li style="list-style-type:disc">ToolNode<ul id="1e2e3b49-c04d-8087-b820-f4c378c946c3" class="bulleted-list"><li style="list-style-type:circle">Creates a graph that works with a chat model that utilizes tool calling.</li></ul></li></ul><hr id="1e1e3b49-c04d-8070-b833-f32ba85cd148"/><p id="1e1e3b49-c04d-8075-92a3-c709a36e2d50" class="">
</p><ul id="1e2e3b49-c04d-8095-9bba-e9973a758051" class="bulleted-list"><li style="list-style-type:disc">Leveraging <a href="https://python.langchain.com/docs/concepts/tool_calling/">tool-calling</a> to interact with a retrieval step has another benefit, which is that  the query for the retrieval is generated by our model. This is  especially important in a conversational setting, where user queries may require contextualization based on the chat history.</li></ul><ul id="1e2e3b49-c04d-8038-b136-c6be08c67129" class="bulleted-list"><li style="list-style-type:disc">3 nodes are involved<blockquote id="1e2e3b49-c04d-80a1-bf6a-efaf5348311d" class=""><ol type="1" id="1e2e3b49-c04d-80ff-8e78-f932b26d378b" class="numbered-list" start="1"><li>A node that fields the user input, either generating a query for the retriever or responding directly;</li></ol><ol type="1" id="1e2e3b49-c04d-80b1-af17-d76c0ea72897" class="numbered-list" start="2"><li>A node for the retriever tool that executes the retrieval step;</li></ol><ol type="1" id="1e2e3b49-c04d-80b3-aa4f-c283d527d8d0" class="numbered-list" start="3"><li>A node that generates the final response using the retrieved context.</li></ol></blockquote><figure id="1e2e3b49-c04d-80c0-81b3-cd4cdebcd2c9" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%207.png"><img style="width:183px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%207.png"/></a></figure></li></ul><p id="1e2e3b49-c04d-80e3-bc35-f3f727e12b72" class="">
</p><p id="1e2e3b49-c04d-8007-9086-cb4ad59d6486" class="">
</p><h1 id="1e2e3b49-c04d-80e5-918f-e36f4832733e" class="">Tools</h1><ul id="1e2e3b49-c04d-801c-81cf-d0e2377276cb" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/tools/">https://python.langchain.com/docs/concepts/tools/</a></li></ul><h2 id="1e2e3b49-c04d-80ac-8e4b-ddff60b52b51" class="">Tool interface</h2><ul id="1e2e3b49-c04d-80dd-a861-de7765a60bf5" class="bulleted-list"><li style="list-style-type:disc">The tool interface is defined in the <a href="https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool">BaseTool</a> class which is a subclass of the <a href="https://python.langchain.com/docs/concepts/runnables/">Runnable Interface</a>.</li></ul><ul id="1e2e3b49-c04d-80de-b340-c5f59f3984c2" class="bulleted-list"><li style="list-style-type:disc">The key attributes that correspond to the tool&#x27;s <strong>schema</strong>:<ul id="1e2e3b49-c04d-8059-8b6d-dbddbc3def03" class="bulleted-list"><li style="list-style-type:circle"><strong>name</strong>: The name of the tool.</li></ul><ul id="1e2e3b49-c04d-804e-adef-cc79784bc405" class="bulleted-list"><li style="list-style-type:circle"><strong>description</strong>: A description of what the tool does.</li></ul><ul id="1e2e3b49-c04d-8053-ba5c-d3fb0edab20c" class="bulleted-list"><li style="list-style-type:circle"><strong>args</strong>: Property that returns the JSON schema for the tool&#x27;s arguments.</li></ul></li></ul><ul id="1e2e3b49-c04d-8052-9f05-e8d382d41492" class="bulleted-list"><li style="list-style-type:disc">The key methods to execute the function associated with the <strong>tool</strong>:<ul id="1e2e3b49-c04d-8016-ab65-f5ef7b1cfa3b" class="bulleted-list"><li style="list-style-type:circle"><strong>invoke</strong>: Invokes the tool with the given arguments.</li></ul><ul id="1e2e3b49-c04d-80cd-afcf-dde352d5c239" class="bulleted-list"><li style="list-style-type:circle"><strong>ainvoke</strong>: Invokes the tool with the given arguments, asynchronously. Used for <a href="https://python.langchain.com/docs/concepts/async/">async programming with Langchain</a>.</li></ul></li></ul><h2 id="1e2e3b49-c04d-80f7-bd25-eb44e920ca21" class="">Tool Artifact</h2><ul id="1e2e3b49-c04d-80e9-943c-ca7238cd1bd4" class="bulleted-list"><li style="list-style-type:disc">This is essentially data or information that we do not want to send the model as a response directly, but it should be available to the downstream task to consume if they want.</li></ul><ul id="1e2e3b49-c04d-80db-b481-e854871f222b" class="bulleted-list"><li style="list-style-type:disc">For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere</li></ul><p id="1e2e3b49-c04d-80eb-946a-f4564cd696c3" class="">
</p><h2 id="1e2e3b49-c04d-8040-9b02-cced4b5e51dd" class="">Trace</h2><figure id="1e2e3b49-c04d-80c1-af11-c164a9cb0a29" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/604e22c0-0c3f-461c-bd72-a5c71644783a.png"><img style="width:401.06666666666666px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/604e22c0-0c3f-461c-bd72-a5c71644783a.png"/></a></figure><p id="1e2e3b49-c04d-8020-900f-ec60607861dd" class="">
</p><p id="1e2e3b49-c04d-8045-bb65-d05bc367cb8e" class="">
</p><h1 id="1e2e3b49-c04d-801a-9bef-dbc64e250401" class="">STATEFUL MANAGEMENT OF CHAT HISTORY</h1><ul id="1e2e3b49-c04d-801b-a035-ebfebe0beed5" class="bulleted-list"><li style="list-style-type:disc">Reading material for deep dive - <a href="https://langchain-ai.github.io/langgraph/concepts/persistence/">https://langchain-ai.github.io/langgraph/concepts/persistence/</a></li></ul><ul id="1e2e3b49-c04d-80af-ae9f-d69b3ce69a87" class="bulleted-list"><li style="list-style-type:disc">Langchain recommends using langgraph as it provides a way to add memory to our application</li></ul><ul id="1e2e3b49-c04d-80d6-b104-de89ab7053ab" class="bulleted-list"><li style="list-style-type:disc">LangGraph has a built-in persistence layer, implemented through checkpointers. <ul id="1e2e3b49-c04d-80c2-9dc8-ef8f3307f4d1" class="bulleted-list"><li style="list-style-type:circle">When you compile graph with a checkpointer, the checkpointer saves a <code>checkpoint</code> of the graph state at every super-step. </li></ul><ul id="1e2e3b49-c04d-80bc-bfef-c8b849984fa5" class="bulleted-list"><li style="list-style-type:circle">Those checkpoints are saved to a <code>thread</code>, which can be accessed after graph execution. </li></ul></li></ul><p id="1e2e3b49-c04d-8048-ba27-da5571aa14f5" class="">
</p><ul id="1e2e3b49-c04d-8051-9dc6-db0f7d43b5af" class="bulleted-list"><li style="list-style-type:disc"><a href="https://langchain-ai.github.io/langgraph/concepts/memory/">https://langchain-ai.github.io/langgraph/concepts/memory/</a></li></ul><p id="1e1e3b49-c04d-80a3-9045-d536ae1b65a6" class="">
</p></li></ol><ol type="1" id="1e1e3b49-c04d-80fd-b172-f9058d95c4a3" class="numbered-list" start="2"><li>Agents<ol type="a" id="1e2e3b49-c04d-8084-916e-e403e2b38e6c" class="numbered-list" start="1"><li>Unlike the chain, wherein once a tool is called it’s response is either used or discarded once the invocation is complete</li></ol><ol type="a" id="1e2e3b49-c04d-800a-8e68-c8313f39b824" class="numbered-list" start="2"><li>Agents can call the tool multiple times.</li></ol><p id="1e2e3b49-c04d-80fa-99f9-ed5c9b3787c5" class="">
</p></li></ol></li></ul></div></details><h1 id="1e2e3b49-c04d-8054-a7b9-d4a7f22dbe90" class="">Agents</h1><ul id="1e2e3b49-c04d-800c-8924-e5f1d88df50b" class="bulleted-list"><li style="list-style-type:disc">By themselves, language models can&#x27;t take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.<ul id="1e2e3b49-c04d-808b-bf71-dfb106fe5ee4" class="bulleted-list"><li style="list-style-type:circle">Prebuilt agents<ul id="1e2e3b49-c04d-80be-a8b9-f45f13a9e303" class="bulleted-list"><li style="list-style-type:square"><a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent">https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent</a></li></ul></li></ul><ul id="1e2e3b49-c04d-80e9-a31e-ff892ca9b683" class="bulleted-list"><li style="list-style-type:circle">Agent architectures<ul id="1e2e3b49-c04d-8045-9246-ea1de2720bb4" class="bulleted-list"><li style="list-style-type:square"><a href="https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/">https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/</a></li></ul></li></ul><ul id="1e2e3b49-c04d-80ac-a20a-d443a610e0b8" class="bulleted-list"><li style="list-style-type:circle"><a href="https://arxiv.org/abs/2210.03629">ReAct</a> is a popular general purpose agent architecture that combines these expansions, integrating three core concepts.<ol type="1" id="1e2e3b49-c04d-8032-948e-f21c268d1f0e" class="numbered-list" start="1"><li><code>Tool calling</code>: Allowing the LLM to select and use various tools as needed.</li></ol><ol type="1" id="1e2e3b49-c04d-80cb-a095-f727db8b4a17" class="numbered-list" start="2"><li><code>Memory</code>: Enabling the agent to retain and use information from previous steps.</li></ol><ol type="1" id="1e2e3b49-c04d-8030-9446-ee13e9045bf9" class="numbered-list" start="3"><li><code>Planning</code>: Empowering the LLM to create and follow multi-step plans to achieve goals.</li></ol></li></ul></li></ul><p id="1e2e3b49-c04d-80ac-af04-c2694d1ed132" class="">
</p><ul id="1e2e3b49-c04d-80e5-aa0f-f3b5947a957b" class="bulleted-list"><li style="list-style-type:disc">Good Article - <a href="https://blog.langchain.dev/how-to-think-about-agent-frameworks/">https://blog.langchain.dev/how-to-think-about-agent-frameworks/</a></li></ul><p id="1e2e3b49-c04d-8093-ad4f-f8a9ba063a3f" class="">
</p><ul id="1e2e3b49-c04d-80a6-b303-fd636e54dcc8" class="bulleted-list"><li style="list-style-type:disc">Using Langchain<ul id="1e2e3b49-c04d-80bf-b84b-de31b22e8eec" class="bulleted-list"><li style="list-style-type:circle"><a href="https://langchain-ai.github.io/langgraph/agents/overview/?ref=blog.langchain.dev#what-is-an-agent">https://langchain-ai.github.io/langgraph/agents/overview/?ref=blog.langchain.dev#what-is-an-agent</a></li></ul></li></ul><p id="1e2e3b49-c04d-80a5-bbed-daddde872819" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>