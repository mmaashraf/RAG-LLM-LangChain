<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>RAG - Build a Retrieval Augmented Generation (RAG) App: Part 1 | �…</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1dbe3b49-c04d-809b-a574-cfd6a9865116" class="page sans"><header><h1 class="page-title">RAG - <a href="https://python.langchain.com/docs/tutorials/rag/">https://python.langchain.com/docs/tutorials/rag/</a></h1><p class="page-description"></p></header><div class="page-body"><p id="1dbe3b49-c04d-8087-b664-ebfcfbb4b01f" class="">
</p><ul id="1dbe3b49-c04d-8095-afeb-ec9bba527fc1" class="bulleted-list"><li style="list-style-type:disc">Implement and explore the concepts that either new or for revision purpose</li></ul><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Part 1</summary><div class="indented"><ul id="1dbe3b49-c04d-806e-ab38-d6b23e9615c5" class="bulleted-list"><li style="list-style-type:disc">rag implementation for q and a over text source</li></ul><ul id="1dbe3b49-c04d-8061-b3d9-fd90de59b452" class="bulleted-list"><li style="list-style-type:disc">using langsmith for tracing</li></ul><p id="1dbe3b49-c04d-800b-832e-f93330e86389" class="">
</p><h3 id="1dbe3b49-c04d-80fb-92be-f6c32bff42fb" class="">Retrieval Techniques</h3><ul id="1dbe3b49-c04d-8010-9159-d571d7c16606" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/retrieval/">https://python.langchain.com/docs/concepts/retrieval/</a></li></ul><ul id="1dbe3b49-c04d-80bf-a50c-cee88227b5cb" class="bulleted-list"><li style="list-style-type:disc">Prerequisites<details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/embedding_models/">https://python.langchain.com/docs/concepts/embedding_models/</a></summary><div class="indented"><ul id="1dbe3b49-c04d-80c5-a68f-d64c596c42d7" class="bulleted-list"><li style="list-style-type:disc">Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text&#x27;s semantic meaning. </li></ul><ul id="1dbe3b49-c04d-8010-b53e-ead4066ee36c" class="bulleted-list"><li style="list-style-type:disc">Leaderboard for a standard benchmark - Massive Text Embedding Benchmark (MTEB) <a href="https://huggingface.co/blog/mteb">here</a> for objective comparisons.</li></ul><ul id="1dbe3b49-c04d-80e3-9895-dcd3dba86265" class="bulleted-list"><li style="list-style-type:disc">BERT used transformer architecture to turn text inputs into vector embedding which provided gains in many of the NLP tasks<ul id="1dbe3b49-c04d-8066-9a22-e2f64f39eaa2" class="bulleted-list"><li style="list-style-type:circle">It was not optimized for sentecen generation tasks.</li></ul><ul id="1dbe3b49-c04d-8071-9916-d7c78398e642" class="bulleted-list"><li style="list-style-type:circle">S-bERT was created.</li></ul></li></ul><ul id="1dbe3b49-c04d-800e-82aa-e76f5e2c4df0" class="bulleted-list"><li style="list-style-type:disc">BERT - <a href="https://www.nvidia.com/en-us/glossary/bert/">https://www.nvidia.com/en-us/glossary/bert/</a><ul id="1dbe3b49-c04d-8075-9822-d8c77a041cb9" class="bulleted-list"><li style="list-style-type:circle">Bidirectional Encoder Representations from Transformers (BERT)</li></ul><ul id="1dbe3b49-c04d-8022-87ec-f3c73b89ef1e" class="bulleted-list"><li style="list-style-type:circle">About left to right, and right to left processing by elMO which processing the entire sentence at one in both directions but does not combine it. whereas BERT unifies it, which leads to better contextual representation.</li></ul><ul id="1dbe3b49-c04d-80fb-a83f-ca0ad3987e1f" class="bulleted-list"><li style="list-style-type:circle">One of the key things in BERT, is language masking where it deletes or masks some of the words.</li></ul><ul id="1dbe3b49-c04d-809b-8954-fc6adb0ab98e" class="bulleted-list"><li style="list-style-type:circle">BERT’s developers solved this problem by masking predicted words as well as other random words in the corpus. BERT also uses a simple training technique of trying to predict whether, given two sentences A and B, B is the antecedent of A or a random sentence.</li></ul><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Directional Processing by elMO and BERT dimensionality of embeddings</summary><div class="indented"><h3 id="1dbe3b49-c04d-803a-a125-d22ad36a3699" class="">1. <strong>Left-to-Right and Right-to-Left Contexts</strong></h3><p id="1dbe3b49-c04d-8015-98be-cdc05c2a716a" class="">In the context of language models like ELMo and BERT, the terms <strong>left-to-right</strong> and <strong>right-to-left</strong> refer to how the model processes the sequence of words in a sentence during training or inference.</p><ul id="1dbe3b49-c04d-80e2-bdf8-e87cf78a9fb6" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-Right</strong> means the model processes the sentence from the <strong>first word</strong> to the <strong>last word</strong>. It only considers words that come before a given word when generating its representation. For example, if the model is processing the word &quot;bat&quot; in the sentence &quot;I hit the ball with a bat,&quot; it would process the words &quot;I hit the ball with a&quot; first, before considering &quot;bat.&quot;</li></ul><ul id="1dbe3b49-c04d-80a8-ac10-e96529c34f38" class="bulleted-list"><li style="list-style-type:disc"><strong>Right-to-Left</strong> means the model processes the sentence from the <strong>last word</strong> to the <strong>first word</strong>. It only considers words that come after a given word when generating its representation. For example, in the same sentence, the model would first process the words &quot;a bat&quot; and then &quot;with the ball hit I&quot; when processing the word &quot;bat.&quot;</li></ul><p id="1dbe3b49-c04d-80a7-bcfb-f080fcb2a011" class="">Here’s a simple illustration:</p><ul id="1dbe3b49-c04d-8062-b27a-d9c074d17eb7" class="bulleted-list"><li style="list-style-type:disc"><strong>Sentence</strong>: &quot;The bat flew away.&quot;</li></ul><ul id="1dbe3b49-c04d-8038-9237-d9f62d2ba0d9" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-right processing</strong> (For &quot;bat&quot;): It would consider: &quot;The&quot; → &quot;bat&quot; → &quot;flew&quot; → &quot;away.&quot;</li></ul><ul id="1dbe3b49-c04d-803b-8d57-ee2c92437ef8" class="bulleted-list"><li style="list-style-type:disc"><strong>Right-to-left processing</strong> (For &quot;bat&quot;): It would consider: &quot;away&quot; → &quot;flew&quot; → &quot;bat&quot; → &quot;The.&quot;</li></ul><p id="1dbe3b49-c04d-808d-9454-cd3ea4695f5c" class=""><strong>ELMo</strong> used separate models for these two directions, while <strong>BERT</strong> uses <strong>bidirectional</strong> processing, meaning it considers both the left-to-right and right-to-left context <strong>at the same time</strong>. This gives a more holistic view of the word in the sentence.</p><h3 id="1dbe3b49-c04d-807e-a08e-d88ff6380bd6" class="">2. <strong>What Drives the Size of Vector Representations?</strong></h3><p id="1dbe3b49-c04d-801b-8500-d0c1d4ba9fb1" class="">The <strong>size</strong> of vector representations, also known as the <strong>dimensionality</strong> of the embedding, is determined by the model architecture and the <strong>number of parameters</strong> chosen for the model.</p><p id="1dbe3b49-c04d-80b3-9163-f172284cd0d4" class="">Several factors that influence the size of vector representations:</p><ul id="1dbe3b49-c04d-80fd-a738-eb1a71fefd03" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Architecture</strong>:<ul id="1dbe3b49-c04d-8018-b2f7-d4fc7b60ab40" class="bulleted-list"><li style="list-style-type:circle">In <strong>Word2Vec</strong> or <strong>GloVe</strong>, the size of the word vector is a hyperparameter you can set. Common choices are 50, 100, 200, or 300 dimensions.</li></ul><ul id="1dbe3b49-c04d-8096-90b4-ef25f41e3f82" class="bulleted-list"><li style="list-style-type:circle">In <strong>ELMo</strong> and <strong>BERT</strong>, the size is determined by the model architecture. For example, BERT-base uses vectors of size <strong>768</strong>, and BERT-large uses vectors of size <strong>1024</strong>.</li></ul></li></ul><ul id="1dbe3b49-c04d-80ff-a466-f673442882b2" class="bulleted-list"><li style="list-style-type:disc"><strong>Hidden Layers</strong>:<ul id="1dbe3b49-c04d-800b-b78c-d5bf798fda41" class="bulleted-list"><li style="list-style-type:circle"><strong>BERT</strong>, being a transformer model, has multiple <strong>layers</strong> (e.g., 12 layers for BERT-base). Each layer processes information differently, and the output at each layer can be represented as a vector.</li></ul><ul id="1dbe3b49-c04d-8003-a61a-f5c50125beb8" class="bulleted-list"><li style="list-style-type:circle">The size of these vectors typically corresponds to the <strong>number of hidden units</strong> in the transformer model. For example, BERT-base has <strong>768 hidden units</strong> per layer, and the final word embedding output (after processing) will also be a vector of size 768.</li></ul></li></ul><ul id="1dbe3b49-c04d-80d6-b393-cbe64016f278" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Capacity</strong>:<ul id="1dbe3b49-c04d-8084-b0cd-d2c9c11ed3c1" class="bulleted-list"><li style="list-style-type:circle">Larger models (like <strong>BERT-large</strong>) tend to have larger vector sizes (e.g., 1024) to capture more complex patterns, but they also require more computation and memory.</li></ul><ul id="1dbe3b49-c04d-8051-bff5-d7c8fc038e9c" class="bulleted-list"><li style="list-style-type:circle">Smaller models may use fewer dimensions (e.g., 256, 512) for efficiency but may not capture as much detail in the word representations.</li></ul></li></ul><h3 id="1dbe3b49-c04d-8062-b91d-e75b328cba7c" class="">In summary:</h3><ul id="1dbe3b49-c04d-8046-9a2d-fc16dd3dccb9" class="bulleted-list"><li style="list-style-type:disc"><strong>Left-to-Right and Right-to-Left</strong> refer to how the model processes words sequentially from different directions. ELMo used these separately, while BERT combines both for a deeper understanding.</li></ul><ul id="1dbe3b49-c04d-8000-80c3-cc467efcb54d" class="bulleted-list"><li style="list-style-type:disc"><strong>Vector Size</strong>: The size of the vector representation (like 768 or 1024 in BERT) is determined by the architecture of the model (number of layers, hidden units, etc.). It impacts the model&#x27;s ability to capture complex patterns but also affects computational requirements.</li></ul></div></details><p id="1dbe3b49-c04d-80a6-a911-d4c9655ca3fe" class="">
</p><ul id="1dbe3b49-c04d-80eb-ae11-cb337d485602" class="bulleted-list"><li style="list-style-type:circle">more detail - <a href="https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/">https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/</a></li></ul><ul id="1dbe3b49-c04d-8055-ab42-f956ea7e7be1" class="bulleted-list"><li style="list-style-type:circle">paper for later - <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></li></ul><hr id="1dbe3b49-c04d-8091-833a-d101b373d2c1"/><ul id="1dbe3b49-c04d-80af-a4b3-eb68420b5f3e" class="bulleted-list"><li style="list-style-type:circle">Langchain offers a common interface, for these model to embed the documents or just text using  <code>embed_documents</code>, <code>embed_query</code> methods</li></ul><ul id="1dbe3b49-c04d-8073-9a17-d9ddae3c54c0" class="bulleted-list"><li style="list-style-type:circle">Langchain integration supports for different providers - <a href="https://python.langchain.com/docs/integrations/text_embedding/">https://python.langchain.com/docs/integrations/text_embedding/</a></li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/retrievers/">Retrievers</a></summary><div class="indented"><ul id="1dbe3b49-c04d-802c-85e8-e255d62c02d0" class="bulleted-list"><li style="list-style-type:disc">The interface takes in an input query which is a string and returns the list of documents <figure id="1dbe3b49-c04d-80da-ba37-f8be26c6d2b7" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image.png"><img style="width:570px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image.png"/></a></figure></li></ul><ul id="1dbe3b49-c04d-809b-83a2-f861e22faeb2" class="bulleted-list"><li style="list-style-type:disc">This class is a <code>Runnable</code> type, which means the Runnable class methods are available and we can use <code>invoke</code> method<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1dbe3b49-c04d-801a-b8a1-ce3537ea159a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1dbe3b49-c04d-80c3-992c-ee3193e2f734" class=""><a href="https://python.langchain.com/docs/how_to/lcel_cheatsheet/">https://python.langchain.com/docs/how_to/lcel_cheatsheet/</a> </p></div></figure></li></ul><ul id="1dbe3b49-c04d-80be-aa52-dbb5f5c37675" class="bulleted-list"><li style="list-style-type:disc">It requires that the method <code>_get_relevant_documents</code> method is implemented.</li></ul><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1dbe3b49-c04d-8086-8137-e522825bb15d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="1dbe3b49-c04d-8030-b5b3-cb923b3e7793" class=""><a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">https://www.pinecone.io/learn/series/langchain/langchain-expression-language/</a><br/>the LECL usage broken down<br/></p><div><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">LECL <a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">LangChain Expression Language</a></summary><div class="indented"><ul id="1dbe3b49-c04d-8026-9716-cc54fef013a8" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.pinecone.io/learn/series/langchain/langchain-expression-language/">https://www.pinecone.io/learn/series/langchain/langchain-expression-language/</a></li></ul><ul id="1dbe3b49-c04d-8073-9638-c3a55c69e1c4" class="bulleted-list"><li style="list-style-type:disc">Traditional syntax for calling a chain<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-80e1-9312-f6580864b1e9" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">chain = LLMChain(
    prompt=prompt,
    llm=model,
    output_parser=output_parser
)</code></pre></li></ul><ul id="1dbe3b49-c04d-801f-a6d0-e674c8f9b17b" class="bulleted-list"><li style="list-style-type:disc"> This uses pipe, which is | operator, it is the <code>__or__</code> method on the class.<ul id="1dbe3b49-c04d-8057-ba21-e27a3a6f11e0" class="bulleted-list"><li style="list-style-type:circle">It take the output from left operations and passes it as input to the neighboring right operations.</li></ul><ul id="1dbe3b49-c04d-807a-811f-ee2f779b6a2e" class="bulleted-list"><li style="list-style-type:circle">Sample example</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-8039-9259-fa01f337d77b" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">def add_five(x):
    return x + 5

def multiply_by_two(x):
    return x * 2

# wrap the functions with Runnable
add_five = Runnable(add_five)
multiply_by_two = Runnable(multiply_by_two)

# run them using the object approach
chain = add_five.__or__(multiply_by_two)
chain(3)  # should return 16</code></pre></li></ul><ul id="1dbe3b49-c04d-8004-a663-dcb31b63c3fe" class="bulleted-list"><li style="list-style-type:disc">A simple RAG pipeline is created.<ul id="1dbe3b49-c04d-8030-b34c-ec1d0a103ee3" class="bulleted-list"><li style="list-style-type:circle">with 2 retrievals, that have half of the information.</li></ul><ul id="1dbe3b49-c04d-80f0-9a04-dd86b61c3f14" class="bulleted-list"><li style="list-style-type:circle">prompt the model using the first retrieval. (it does not have the data)</li></ul></li></ul><ul id="1dbe3b49-c04d-8079-912d-cb793bdf0baa" class="bulleted-list"><li style="list-style-type:disc">New concepts<figure id="1dbe3b49-c04d-80c9-b924-d7965c0edf45" class="image"><a href="LangChain%201c4e3b49c04d800d8533f6985c38fae5/image.png"><img style="width:3442px" src="LangChain%201c4e3b49c04d800d8533f6985c38fae5/image.png"/></a></figure><ul id="1dbe3b49-c04d-80e1-9717-df8b6ebedf0d" class="bulleted-list"><li style="list-style-type:circle">RunnableParallel<ul id="1dbe3b49-c04d-8085-b308-d4942c5117ce" class="bulleted-list"><li style="list-style-type:square"><code>RunnableParallel</code> object allows us to define multiple values and operations, and run them all in parallel. </li></ul></li></ul><ul id="1dbe3b49-c04d-8025-a290-d0ca3fae7c63" class="bulleted-list"><li style="list-style-type:circle">RunnablePassThrough<ul id="1dbe3b49-c04d-803f-ad20-c7f0f37af4d7" class="bulleted-list"><li style="list-style-type:square">The <code>RunnablePassthrough</code> object is used as a &quot;passthrough&quot; take takes any input to the current component (retrieval) and allows us to provide it in the component output via the &quot;question&quot; key.</li></ul></li></ul></li></ul><ul id="1dbe3b49-c04d-8050-99ae-d149621ff196" class="bulleted-list"><li style="list-style-type:disc">when the chain that is defined using the pipe operator is then invoked. using on vector_a as the context. The llm does not have an answer. which is expected.<ul id="1dbe3b49-c04d-8068-b4c0-c520f03c9063" class="bulleted-list"><li style="list-style-type:circle">Fortunately, <code>Runnable Parallel</code> allows us to provide multiple contexts, and this time we can pass both halves of the information.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-8022-ab5d-e3c1131e8103" class="code"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">prompt_str = &quot;&quot;&quot;Answer the question below using the context:

Context:
{context_a}
{context_b}

Question: {question}

Answer: &quot;&quot;&quot;
prompt = ChatPromptTemplate.from_template(prompt_str)

retrieval = RunnableParallel(
    {
        &quot;context_a&quot;: retriever_a, &quot;context_b&quot;: retriever_b,
        &quot;question&quot;: RunnablePassthrough()
    }
)

chain = retrieval | prompt | model | output_parser</code></pre></li></ul><ul id="1dbe3b49-c04d-802f-94b3-e1a6504001a6" class="bulleted-list"><li style="list-style-type:disc">The <code>RunnableLambda</code> is a LangChain abstraction that allows us to turn Python functions into pipe-compatible functions,<ul id="1dbe3b49-c04d-808e-8ca4-d4d7feeb1b2a" class="bulleted-list"><li style="list-style-type:circle">THE PIPE OPERATOR CAN BE USED WHEN A FUNCTION IS CONVERTED INTO <code>RUNNABLELAMBDA</code></li></ul></li></ul><p id="1dbe3b49-c04d-8096-9645-e1a5dfa3ba71" class="">
</p><p id="1dbe3b49-c04d-801b-bd95-fe65941d2dfb" class="">
</p></div></details></div></div></figure><ul id="1dbe3b49-c04d-8028-8de5-cfd3fe837401" class="bulleted-list"><li style="list-style-type:disc">Retrievers do not store documents</li></ul><ul id="1dbe3b49-c04d-80aa-ba96-d7b9cf460cda" class="bulleted-list"><li style="list-style-type:disc">Common types of retrieval systems<ul id="1dbe3b49-c04d-8071-ac50-d464f4d15c1a" class="bulleted-list"><li style="list-style-type:circle">APIs</li></ul><ul id="1dbe3b49-c04d-8051-baaa-f705ff80b5b2" class="bulleted-list"><li style="list-style-type:circle">RDBMS<ul id="1dbe3b49-c04d-80e7-9b30-f7c4eb8889b4" class="bulleted-list"><li style="list-style-type:square">In these cases, <a href="https://python.langchain.com/docs/concepts/retrieval/">query analysis</a> techniques to construct a structured query from natural language is critical.</li></ul><ul id="1dbe3b49-c04d-80b5-bb45-d3c3713a97a5" class="bulleted-list"><li style="list-style-type:square">For example, you can build a retriever for a SQL database using <br/>text-to-SQL conversion. This allows a natural language query (string) <br/>retriever to be transformed into a SQL query behind the scenes.<br/></li></ul></li></ul><ul id="1dbe3b49-c04d-8064-9060-f25eb8d165b1" class="bulleted-list"><li style="list-style-type:circle">Lexical Search<ul id="1dbe3b49-c04d-80c7-83d8-ee27e403aa5b" class="bulleted-list"><li style="list-style-type:square">Many search engine use lexical search, words in the query to match with words in the documents</li></ul><ul id="1dbe3b49-c04d-8076-b64e-f714f983aeb3" class="bulleted-list"><li style="list-style-type:square">Example: TF-IDF, or BM-25</li></ul><ul id="1dbe3b49-c04d-80fe-a2cf-f406590ff574" class="bulleted-list"><li style="list-style-type:square">There’ elastic search integration available for langchain</li></ul></li></ul><ul id="1dbe3b49-c04d-80bf-86b8-eacfbdbb57ca" class="bulleted-list"><li style="list-style-type:circle"> Vector Store<ul id="1dbe3b49-c04d-8078-973f-ea80406e1388" class="bulleted-list"><li style="list-style-type:square">Powerful when dealing with unstructured data</li></ul><ul id="1dbe3b49-c04d-8062-b866-d958ec7eac08" class="bulleted-list"><li style="list-style-type:square">You can convert a vector store in a retrieval using the <code>as_retriever()</code> method</li></ul></li></ul></li></ul><ul id="1dbe3b49-c04d-8070-897b-d442fe1540c0" class="bulleted-list"><li style="list-style-type:disc">Advanced search techniques<ul id="1dbe3b49-c04d-8021-8595-dcbb6e6f6804" class="bulleted-list"><li style="list-style-type:circle">Ensemble<ul id="1dbe3b49-c04d-80a6-ad14-db515f8d0b6e" class="bulleted-list"><li style="list-style-type:square">It involves combines results of different retrievals that are good at finding different aspects of the query with the documents.</li></ul><ul id="1dbe3b49-c04d-8067-a789-e9aee01d566e" class="bulleted-list"><li style="list-style-type:square">pass the retrievers in the list as input, specify the weights for each one.</li></ul><ul id="1dbe3b49-c04d-80fd-b2c1-cc7955ebd4e5" class="bulleted-list"><li style="list-style-type:square">Finally, reranking is done, using another algorithm</li></ul></li></ul><ul id="1dbe3b49-c04d-8062-8806-f025329fd505" class="bulleted-list"><li style="list-style-type:circle">Source Document retention<ul id="1dbe3b49-c04d-80c0-acdc-ffdcf8080401" class="bulleted-list"><li style="list-style-type:square">The main idea is — after the indexing is done, being able to trace back to the original document .</li></ul><ul id="1dbe3b49-c04d-806d-870a-e6698c6b63b9" class="bulleted-list"><li style="list-style-type:square">2 types<ul id="1dbe3b49-c04d-8006-a824-daf6766b9634" class="bulleted-list"><li style="list-style-type:disc">ParentDocument <ul id="1dbe3b49-c04d-804f-96ee-f988871211e4" class="bulleted-list"><li style="list-style-type:circle">retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.</li></ul></li></ul><ul id="1dbe3b49-c04d-80b2-9888-e618945d07f3" class="bulleted-list"><li style="list-style-type:disc">MultiVector<ul id="1dbe3b49-c04d-8048-9b00-dfd378dd0581" class="bulleted-list"><li style="list-style-type:circle">This may use LLM to store different transformation of the source document, perhaps captures things that provide various aspects of the source, using hypothetical questions or it can just be summary.</li></ul></li></ul></li></ul></li></ul></li></ul><p id="1dbe3b49-c04d-805e-a3c7-dc9603582217" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/vectorstores/">Vector stores</a></summary><div class="indented"><ul id="1dbe3b49-c04d-808b-8862-c5d81b1b9b07" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/concepts/embedding_models/">Embeddings</a></li></ul><ul id="1dbe3b49-c04d-80b1-902a-e449d8a52571" class="bulleted-list"><li style="list-style-type:disc">These are used to index documents(unstructured) into vector representation ( called embeddings) and then use for efficient retrieval for a given query.<figure id="1dbe3b49-c04d-80bc-913b-eb21f925f70c" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%201.png"><img style="width:570.0064697265625px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%201.png"/></a></figure></li></ul><p id="1dbe3b49-c04d-80ef-bc47-d25e8f0938c3" class="">
</p><ul id="1dbe3b49-c04d-80d3-a4b4-fe94ac86452a" class="bulleted-list"><li style="list-style-type:disc">LangChain provides a standard interface for working with vector <br/>stores, allowing users to easily switch between different vectorstore <br/>implementations.<br/></li></ul><ul id="1dbe3b49-c04d-80b6-9b83-f34aa303f3c5" class="bulleted-list"><li style="list-style-type:disc">The interface consists of basic methods for writing, deleting and searching for documents in the vector store.</li></ul><ul id="1dbe3b49-c04d-80f5-8d1d-fc5cf0eea74d" class="bulleted-list"><li style="list-style-type:disc">The key methods are:<ul id="1dbe3b49-c04d-8053-b7b3-ebf41992859d" class="bulleted-list"><li style="list-style-type:circle"><code>add_documents</code>: Add a list of texts to the vector store.</li></ul><ul id="1dbe3b49-c04d-8033-b196-d3faac4db19b" class="bulleted-list"><li style="list-style-type:circle"><code>delete</code>: Delete a list of documents from the vector store.</li></ul><ul id="1dbe3b49-c04d-8069-a4d4-c7f9b51822c6" class="bulleted-list"><li style="list-style-type:circle"><code>similarity_search</code>: Search for similar documents to a given query.</li></ul></li></ul><ul id="1dbe3b49-c04d-8038-9c1f-cfa3b376fc73" class="bulleted-list"><li style="list-style-type:disc">Provide the document ids when adding or deleting documents, as it makes the operations efficient</li></ul><ul id="1dbe3b49-c04d-803f-afdd-f58ffd0b05ce" class="bulleted-list"><li style="list-style-type:disc">Similarity Search<ul id="1dbe3b49-c04d-8061-81f5-ef3f911fe869" class="bulleted-list"><li style="list-style-type:circle">Now that documents are represented as some Vectors. Given a query in text format, is it converted into an embedding and a similarity check algorithm is run to score the matches and then rank the results.</li></ul><ul id="1dbe3b49-c04d-80d9-ac6f-d4cefcf8e6c8" class="bulleted-list"><li style="list-style-type:circle">Score or Metrics<ul id="1dbe3b49-c04d-8035-8022-f17cf1c74a3b" class="bulleted-list"><li style="list-style-type:square">Cosine</li></ul><ul id="1dbe3b49-c04d-805f-8890-e647f6d3e08e" class="bulleted-list"><li style="list-style-type:square">Dot product</li></ul><ul id="1dbe3b49-c04d-803a-9952-ff9a50cf5747" class="bulleted-list"><li style="list-style-type:square">Euclidean distance</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dbe3b49-c04d-80e0-9c64-fb2a069d4379" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">## example code snippets

from langchain_core.vectorstores import InMemoryVectorStore
# Initialize with an embedding model
vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())

from langchain_core.documents import Document

document_1 = Document(
    page_content=&quot;I had chocalate chip pancakes and scrambled eggs for breakfast this morning.&quot;,
    metadata={&quot;source&quot;: &quot;tweet&quot;},
)

document_2 = Document(
    page_content=&quot;The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.&quot;,
    metadata={&quot;source&quot;: &quot;news&quot;},
)

documents = [document_1, document_2]

vector_store.add_documents(documents=documents)

vector_store.add_documents(documents=documents, ids=[&quot;doc1&quot;, &quot;doc2&quot;])
vector_store.delete(ids=[&quot;doc1&quot;])

query = &quot;my query&quot;
docs = vectorstore.similarity_search(query)</code></pre></li></ul><p id="1dbe3b49-c04d-8035-bd38-eed0a8e86317" class="">
</p><ul id="1dbe3b49-c04d-807f-b370-e4487a926017" class="bulleted-list"><li style="list-style-type:disc">MetaData filtering<ul id="1dbe3b49-c04d-8057-9ef9-f7f0698cccd1" class="bulleted-list"><li style="list-style-type:circle">the interface offers filtering based on properties available in the documents. These are provided during creation of documents, as the metadata argument</li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><a href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a></summary><div class="indented"><ul id="1dbe3b49-c04d-80ab-a990-f1e5c2b638d5" class="bulleted-list"><li style="list-style-type:disc">Splitting is done for the following reasons<ul id="1dbe3b49-c04d-80b6-b2e3-dcaf545531e7" class="bulleted-list"><li style="list-style-type:circle">Limited windows size of the model</li></ul><ul id="1dbe3b49-c04d-8052-9876-cecc60232ae3" class="bulleted-list"><li style="list-style-type:circle">consistent length of documents</li></ul><ul id="1dbe3b49-c04d-806e-ae21-c4a6a3932b23" class="bulleted-list"><li style="list-style-type:circle">optimizing training performance</li></ul><ul id="1dbe3b49-c04d-806e-8af6-d92a38116820" class="bulleted-list"><li style="list-style-type:circle">improves the quality of vectors/representation </li></ul></li></ul><ul id="1dbe3b49-c04d-8060-a610-e0be8cf35602" class="bulleted-list"><li style="list-style-type:disc">Types<ul id="1dbe3b49-c04d-803b-a496-ca2c6c7686c9" class="bulleted-list"><li style="list-style-type:circle">Length<ul id="1dbe3b49-c04d-8086-86c1-fc2c77f7cfe8" class="bulleted-list"><li style="list-style-type:square">Number of characters, or tokens</li></ul></li></ul><ul id="1dbe3b49-c04d-806a-a4b9-de28d18dd248" class="bulleted-list"><li style="list-style-type:circle">Text <ul id="1dbe3b49-c04d-801e-aceb-fe2f8f4c888d" class="bulleted-list"><li style="list-style-type:square">It tries to keep related information together, like a paragraph, sentence, words. </li></ul></li></ul><ul id="1dbe3b49-c04d-8053-bb65-d7c023d73432" class="bulleted-list"><li style="list-style-type:circle">Document<ul id="1dbe3b49-c04d-80d6-81bb-cbeaaa1cfa1f" class="bulleted-list"><li style="list-style-type:square">If it;s a markdown, then split based on the different tags it uses.</li></ul><ul id="1dbe3b49-c04d-809a-b895-d1c54d44b672" class="bulleted-list"><li style="list-style-type:square">JSON, HTML, </li></ul></li></ul><ul id="1dbe3b49-c04d-808a-bbab-e6b6b67d1578" class="bulleted-list"><li style="list-style-type:circle">Semantic<ul id="1dbe3b49-c04d-8056-a7db-eabf14d612f1" class="bulleted-list"><li style="list-style-type:square">While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text&#x27;s semantics. </li></ul></li></ul></li></ul></div></details></li></ul><p id="1dbe3b49-c04d-80db-adc1-f90044583e33" class="">
</p><h3 id="1dbe3b49-c04d-806e-8027-df2494029aa3" class="">Semantic Search</h3><ul id="1dbe3b49-c04d-802f-83b1-c7770de98bc1" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/tutorials/retrievers/">https://python.langchain.com/docs/tutorials/retrievers/</a></li></ul><ul id="1dbe3b49-c04d-8098-a8a0-e9b0546122ba" class="bulleted-list"><li style="list-style-type:disc">This whole thing can be broken into the following steps<ol type="1" id="1dbe3b49-c04d-80f4-8d49-e7ab7b3f582b" class="numbered-list" start="1"><li>loading<ol type="a" id="1dbe3b49-c04d-80ae-b6e3-f034252e9818" class="numbered-list" start="1"><li>use document loaders</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8000-ba34-c01f03fb510e" class="numbered-list" start="2"><li>breaking into chunks<ol type="a" id="1dbe3b49-c04d-801f-8da4-dbed24279ca4" class="numbered-list" start="1"><li>some sort of splitters</li></ol><ol type="a" id="1dbe3b49-c04d-80f4-b615-def15309e801" class="numbered-list" start="2"><li>to ensure it fits into the model’s finite context windows size</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8027-8b30-e173c033e702" class="numbered-list" start="3"><li>Storing into vector dbs<figure id="1dbe3b49-c04d-80b8-b762-e9ea96c11f34" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%202.png"><img style="width:2583px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%202.png"/></a></figure></li></ol><ol type="1" id="1dbe3b49-c04d-80cb-b10b-e906ecb99c38" class="numbered-list" start="4"><li>retrieval:<ol type="a" id="1dbe3b49-c04d-801b-9538-cb0b483bca0a" class="numbered-list" start="1"><li>given a user input, retrieves the relevant splits</li></ol></li></ol><ol type="1" id="1dbe3b49-c04d-8076-b8fc-f8220efa4815" class="numbered-list" start="5"><li>generate<ol type="a" id="1dbe3b49-c04d-8067-866c-d75783ce9405" class="numbered-list" start="1"><li>user prompt, along with the document as the input to the models to generate an answer</li></ol><figure id="1dbe3b49-c04d-8081-a71a-d216ae89a09e" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%203.png"><img style="width:2532px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%203.png"/></a></figure></li></ol></li></ul><ul id="1dbe3b49-c04d-8064-81ff-e107b8787b3a" class="bulleted-list"><li style="list-style-type:disc">Once we&#x27;ve indexed our data, we will use <a href="https://langchain-ai.github.io/langgraph/">LangGraph</a> as our orchestration framework to implement the retrieval and generation steps.</li></ul></div></details><p id="1dce3b49-c04d-8082-b7ec-f8ecd9b9bc42" class="">
</p><h1 id="1dce3b49-c04d-8026-8bc4-ea3d4105129e" class="">Coding Part 1</h1><ul id="1dce3b49-c04d-80b6-919e-da3f10dcd98c" class="bulleted-list"><li style="list-style-type:disc">After getting the langsmith setup ready<ul id="1dce3b49-c04d-80df-ad8a-f75e332984e3" class="bulleted-list"><li style="list-style-type:circle">install packages</li></ul><ul id="1dce3b49-c04d-8073-b683-cfc3499e0139" class="bulleted-list"><li style="list-style-type:circle">Store the keys</li></ul></li></ul><ul id="1dce3b49-c04d-80ba-91dd-d90be5a0715a" class="bulleted-list"><li style="list-style-type:disc">Now, I require 3 components<ul id="1dce3b49-c04d-806a-90d7-dcac7ae71ec0" class="bulleted-list"><li style="list-style-type:circle">Model</li></ul><ul id="1dce3b49-c04d-80c1-b40a-d6e4686d113c" class="bulleted-list"><li style="list-style-type:circle">Embedding</li></ul><ul id="1dce3b49-c04d-800c-97a4-d07558a5d08c" class="bulleted-list"><li style="list-style-type:circle">Vectorstore</li></ul></li></ul><ul id="1dce3b49-c04d-80c0-827c-daad632bd3d1" class="bulleted-list"><li style="list-style-type:disc">The corresponding commands are<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1dce3b49-c04d-8093-aebe-e9c5f7589e33" class="code"><code class="language-JavaScript">pip install -qU &quot;langchain[openai]&quot;
pip install -U langchain langchain-openai
pip install -qU langchain-core</code></pre></li></ul><p id="1dce3b49-c04d-8020-8439-e698cadfcfde" class="">
</p><h3 id="1dce3b49-c04d-80ec-a83b-c42b08fecf5f" class="">GOAL</h3><ul id="1dce3b49-c04d-80d7-a634-cfd6dd63c7c6" class="bulleted-list"><li style="list-style-type:disc">build an app that answers questions about the website&#x27;s content. The specific website we will use is the <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a> blog post by Lilian Weng, which allows us to ask questions about the contents of the post.</li></ul><p id="1dce3b49-c04d-807e-b7b7-d1de1d8be10c" class="">
</p><ul id="1dce3b49-c04d-8022-bba7-c101e81082a0" class="bulleted-list"><li style="list-style-type:disc">Update the LLM init code to use the langsmith</li></ul><ul id="1dce3b49-c04d-80fb-9aae-c78802989921" class="bulleted-list"><li style="list-style-type:disc">Trace output</li></ul><ul id="1e1e3b49-c04d-80bb-beda-d624022c9b34" class="bulleted-list"><li style="list-style-type:disc"><a href="https://smith.langchain.com/">https://smith.langchain.com/</a><figure id="1dce3b49-c04d-80dc-b6c1-f5b0d66f6b42" class="image"><a href="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%204.png"><img style="width:2866px" src="RAG%20-%20Build%20a%20Retrieval%20Augmented%20Generation%20(RAG)%201dbe3b49c04d809ba574cfd6a9865116/image%204.png"/></a></figure></li></ul><ul id="1dce3b49-c04d-8085-8c9a-c362a55375b8" class="bulleted-list"><li style="list-style-type:disc">Relation between Embedding and Model<ul id="1e1e3b49-c04d-80fb-93fc-d144a208e3b8" class="bulleted-list"><li style="list-style-type:circle">In essence: The embedding model provides the &quot;language&quot; for the RAG system to understand and compare information, while the LLM (and potentially rerankers) handle <br/>the retrieval and generation aspects.<br/></li></ul></li></ul><details open=""><summary style="font-weight:600;font-size:1.5em;line-height:1.3;margin:0">Purpose of some code blocks</summary><div class="indented"><h2 id="1e1e3b49-c04d-8005-8b3f-f8fa8f7526a2" class="">Indexing</h2><ul id="1e1e3b49-c04d-800f-8308-f3d42e13f45f" class="bulleted-list"><li style="list-style-type:disc">In this case, WebBaseLoader, is used to load a webpage, and we use soup to focus on relavent tags such as <strong>“post-content”, “post-title”, or “post-header”</strong><blockquote id="1e1e3b49-c04d-8082-917e-f9553c60ad26" class=""><strong>In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs).</strong></blockquote></li></ul><ul id="1e1e3b49-c04d-8028-bc28-e05425183809" class="bulleted-list"><li style="list-style-type:disc">The splits are to be converted into embeddings, this is done by added them to the vector store<ul id="1e1e3b49-c04d-806c-b801-ca8c495ae70d" class="bulleted-list"><li style="list-style-type:circle">in this case, InMemoryDataStore is being used</li></ul><ul id="1e1e3b49-c04d-80b0-a38c-f769189c4dfb" class="bulleted-list"><li style="list-style-type:circle">along with OpenAI embedding<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1e3b49-c04d-80d3-bd69-e0e0bd172e65" class="code"><code class="language-JavaScript">embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-large&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1e3b49-c04d-808e-b340-e1162b8de96a" class="code"><code class="language-JavaScript">vector_store.add_documents(documents=all_splits)</code></pre></li></ul></li></ul><ul id="1e1e3b49-c04d-80d9-af4b-c0b5c79f022a" class="bulleted-list"><li style="list-style-type:disc">At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.<hr id="1e1e3b49-c04d-80e6-9f87-c0753890e5ae"/></li></ul><ul id="1dce3b49-c04d-80c4-b7fc-f214edcffab6" class="bulleted-list"><li style="list-style-type:disc">About the prompt using hub.pull<ul id="1e1e3b49-c04d-8006-96c2-ce0ab59605bc" class="bulleted-list"><li style="list-style-type:circle"><a href="https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub">https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub</a><ul id="1e1e3b49-c04d-8096-a6fc-cf7cfd47b37d" class="bulleted-list"><li style="list-style-type:square">available prompts that can be pulled to chat with the LLM.</li></ul><ul id="1e1e3b49-c04d-8088-81b6-f05185defcc0" class="bulleted-list"><li style="list-style-type:square">Depending on the usecase, different prompts are available. <ul id="1e1e3b49-c04d-80e4-be3b-e9c7fbb5b51b" class="bulleted-list"><li style="list-style-type:disc">Since, we are trying to build rag, using <a href="https://smith.langchain.com/hub/rlm/rag-prompt">https://smith.langchain.com/hub/rlm/rag-prompt</a>, to predefine things for us</li></ul><ul id="1e1e3b49-c04d-8098-ab4a-d546b66f7252" class="bulleted-list"><li style="list-style-type:disc">it defines the question, context that are going to be passed in the prompt, and the expected output from the LLM</li></ul></li></ul></li></ul><ul id="1e1e3b49-c04d-8094-806f-ecf7dcd683db" class="bulleted-list"><li style="list-style-type:circle"><code>StateGraph</code> is part of the langchain graph module. It helps using a State, which is essentially a dictionary, with the keys being the variable/placeholders that are being used by the LLM (originating from the prompt that’s defined)<ul id="1e1e3b49-c04d-8065-b90c-e974d9bfb993" class="bulleted-list"><li style="list-style-type:square">defines the order of operations</li></ul><ul id="1e1e3b49-c04d-8071-baf0-d2fd5fbc4d67" class="bulleted-list"><li style="list-style-type:square">and add edges</li></ul><ul id="1e1e3b49-c04d-80ac-aba6-c475535467c1" class="bulleted-list"><li style="list-style-type:square">compile, so that it becomes a Runnable object</li></ul></li></ul></li></ul><h2 id="1e1e3b49-c04d-8002-a5b3-f0f76e9e0b73" class="">Retrieval and Generation</h2><ul id="1e1e3b49-c04d-801c-b925-df378bfa54c5" class="bulleted-list"><li style="list-style-type:disc">To use LangGraph, we need to define three things:<ol type="1" id="1e1e3b49-c04d-8072-b3a2-dee333ed899e" class="numbered-list" start="1"><li>The state of our application;</li></ol><ol type="1" id="1e1e3b49-c04d-800f-840d-ecdd2d7ee82e" class="numbered-list" start="2"><li>The nodes of our application (i.e., application steps);</li></ol><ol type="1" id="1e1e3b49-c04d-8036-95bf-d1ac640b751f" class="numbered-list" start="3"><li>The &quot;control flow&quot; of our application (e.g., the ordering of the steps).</li></ol><h3 id="1e1e3b49-c04d-80e6-81d7-db31c14275ed" class="">State: </h3><ul id="1e1e3b49-c04d-8079-8a29-c9cdc6375580" class="bulleted-list"><li style="list-style-type:circle">of our application controls what data is input to the application, transferred between steps, and output by the application. </li></ul><ul id="1e1e3b49-c04d-80b9-88d9-c0958fb25497" class="bulleted-list"><li style="list-style-type:circle">It is typically a <code>TypedDict</code>, but can also be a <a href="https://langchain-ai.github.io/langgraph/how-tos/state-model/">Pydantic BaseModel</a></li></ul></li></ul><p id="1e1e3b49-c04d-8094-ba2a-c97b9aa8087c" class="">
</p><h2 id="1e1e3b49-c04d-8022-9070-dfffed417b16" class="">Query Analysis</h2><ul id="1e1e3b49-c04d-8024-855d-f468d161fe20" class="bulleted-list"><li style="list-style-type:disc">The state can be extended to define the output structure using a pydantic model or typedDict</li></ul><ul id="1e1e3b49-c04d-80f6-87f1-cb69a83f1711" class="bulleted-list"><li style="list-style-type:disc">If pydantic model is used, the return type will be a runnable object else a json string format.</li></ul><ul id="1e1e3b49-c04d-803b-9a0b-e7922959eafa" class="bulleted-list"><li style="list-style-type:disc">Add it as a step before the retrival</li></ul><ul id="1e1e3b49-c04d-804f-af3f-d68dcde5c4f9" class="bulleted-list"><li style="list-style-type:disc">- <a href="https://python.langchain.com/docs/how_to/structured_output/">https://python.langchain.com/docs/how_to/structured_output/</a></li></ul><ul id="1e1e3b49-c04d-80d8-a220-c7ebe4ee250c" class="bulleted-list"><li style="list-style-type:disc">Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. F</li></ul><ul id="1e1e3b49-c04d-80da-9758-ef2be84d2947" class="bulleted-list"><li style="list-style-type:disc"><a href="https://python.langchain.com/docs/how_to/#query-analysis">https://python.langchain.com/docs/how_to/#query-analysis</a></li></ul><p id="1e1e3b49-c04d-800d-ac8a-cfe59229a02f" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Part 2 </summary><div class="indented"><ul id="1dbe3b49-c04d-80fb-969e-c50007ba4693" class="bulleted-list"><li style="list-style-type:disc">conversation-style interactions and multi-step retrieval processes.</li></ul></div></details><p id="1e1e3b49-c04d-8021-a949-c4b96b56756d" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>